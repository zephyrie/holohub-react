[
    {
        "metadata": {
            "name": "Advanced Networking Benchmark",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Network",
                "Networking",
                "DPDK",
                "UDP",
                "Ethernet",
                "IP",
                "GPUDirect",
                "RDMA"
            ],
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "advanced_networking_benchmark",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/adv_networking_bench",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Advanced Networking Benchmark\n\nThis is a simple application to measure a lower bound on performance for the advanced networking operator\nby receiving packets, optionally doing work on them, and freeing the buffers. While only freeing the packets is\nan unrealistic workload, it's useful to see at a high level whether the application is able to keep up with\nthe bare minimum amount of work to do. The application contains both a transmitter and receiver that are\ndesigned to run on different systems, and may be configured independently.\n\nThe performance of this application depends heavily on a properly-configured system and choosing the best\ntuning parameters that are acceptable for the workload. To configure the system please see the documentation\nfor the advanced network operator. With the system tuned, the application performance will be dictated\nby batching size and whether GPUDirect is enabled. \n\nAt this time both the transmitter and receiver are written to handle an Ethernet+IP+UDP packet with a\nconfigurable payload. Other modes may be added in the future. Also, for simplicity, the transmitter and\nreceiver must be configured to a single packet size.\n\n## Transmit\n\nThe transmitter sends a UDP packet with an incrementing sequence of bytes after the UDP header. The batch\nsize configured dictates how many packets the benchmark operator sends to the advanced network operator\nin each tick. Typically with the same number of CPU cores the transmitter will run faster than the receiver, \nso this parameter may be used to throttle the sender somewhat by making the batches very small.\n\n## Receiver\n\nThe receiver receives the UDP packets in either CPU-only mode or header-data split mode. CPU-only mode\nwill receive the packets in CPU memory, copy the payload contents to a host-pinned staging buffer, and\nfreed. In header-data split mode the user may configure a split point where the bytes before that point\nare sent to the CPU, and all bytes afterwards are sent to the GPU. Header-data split should achieve higher\nrates than CPU mode since the amount of data to the CPU can be orders of magnitude lower compared to running\nin CPU-only mode. \n\n### Configuration\n\nThe application is configured using a separate transmit and receive file. The transmit file is called\n`adv_networking_bench_tx.yaml` while the receive is named `adv_networking_bench_rx.yaml`. Configure the\nadvanced networking operator on both transmit and receive per the instructions for that operator.\n\n#### Receive Configuration\n\n- `header_data_split`: bool\n  Turn on GPUDirect header-data split mode\n- `batch_size`: integer\n  Size in packets for a single batch. This should be a multiple of the advanced network RX operator batch size.\n  A larger batch size consumes more memory since any work will not start unless this batch size is filled. Consider\n  reducing this value if errors are occurring.\n- `max_packet_size`: integer\n  Maximum packet size expected. This value includes all headers up to and including UDP.\n\n#### Transmit Configuration\n\n- `batch_size`: integer\n  Size in packets for a single batch. This batch size is used to send to the advanced network TX operator, and \n  will loop sending that many packets for each burst.\n- `payload_size`: integer\n  Size of the payload to send after all L2-L4 headers \n\n### Requirements\n\nThis application requires all configuration and requirements from the advanced network operator.\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n### Run Instructions\n\nFirst, go in your `build` or `install` directory, then for the transmitter run:\n\n\n```bash\n./build/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_tx.yaml\n```\n\nOr for the receiver:\n\n```bash\n./build/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_tx.yaml\n```\n",
        "application_name": "adv_networking_bench",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Software Defined Radio FM Demodulation",
            "authors": [
                {
                    "name": "Adam Thompson",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk_version": "0.4.0",
            "platforms": [
                "amd64"
            ],
            "tags": [
                "Communications",
                "Aerospace",
                "Defence",
                "Lifesciences"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "numpy",
                        "version": "1.23.2"
                    },
                    {
                        "name": "cupy",
                        "version": "11.4"
                    },
                    {
                        "name": "cusignal",
                        "version": "22.12"
                    },
                    {
                        "name": "SoapySDR",
                        "version": "0.8.1"
                    },
                    {
                        "name": "soapysdr-module-rtlsdr",
                        "version": "0.3"
                    },
                    {
                        "name": "pyaudio",
                        "version": "0.2.13"
                    }
                ]
            },
            "run": {
                "command": "python3 sdr_fm_demodulation.py",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# SDR FM Demodulation Application\n\nAs the \"Hello World\" application of software defined radio developers, this demonstration highlights real-time FM demodulation, resampling, and playback on GPU with NVIDIA's Holoscan SDK. In this example, we are using an inexpensive USB-based [RTL-SDR](https://www.rtl-sdr.com/) dongle to feed complex valued Radio Frequency (RF) samples into GPU memory and use [cuSignal](https://github.com/rapidsai/cusignal) functions to perform the relevant signal processing. The main objectives of this demonstration are to:\n- Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries\n- Demonstrate how to construct and connect isolated units of work via Holoscan operators\n- Emphasize that operators created for this application can be re-used in other ones doing similar tasks\n\n# Running the Application\n\nPrior to running the application, the user needs to install the necessary dependencies (and, of course, plug in a SDR into your computer). This is most easily done in an Anaconda environment.\n\n```\nconda create --name holoscan-sdr-demo python=3.8\nconda activate holoscan-sdr-demo\nconda install -c conda-forge -c rapidsai -c nvidia cusignal soapysdr soapysdr-module-rtlsdr pyaudio\npip install holoscan\n```\n\nThe FM demodulation example can then be run via\n```\npython applications/sdr_fm_demodulation/sdr_fm_demodulation.py\n```\n",
        "application_name": "sdr_fm_demodulation",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Videomaster transmitter example",
            "authors": [
                {
                    "name": "Laurent Radoux",
                    "affiliation": "DELTACAST"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk_version": "0.5.0",
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Deltacast",
                "VideoMaster"
            ],
            "ranking": 2,
            "dependencies": {},
            "run": {
                "command": "<holohub_app_bin>/deltacast_transmitter --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Deltacast transmitter\n\nThis application demonstrates the use of videomaster_transmitter to transmit a video stream through a dedicated IO device.\n\n### Requirements\n\nThis application uses the DELTACAST.TV capture card for input stream. Contact [DELTACAST.TV](https://www.deltacast.tv/) for more details on how access the SDK and to setup your environment.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nSee instructions from the top level README on how to build this application.\nNote that this application requires to provide the VideoMaster_SDK_DIR if it is not located in a default location on the system.\nThis can be done with the following command, from the top level Holohub source directory:\n\n```bash\n./run build deltacast_transmitter --configure-args -DVideoMaster_SDK_DIR=<Path to VideoMasterSDK>\n```\n\n### Run Instructions\n\nFrom the build directory, run the command:\n\n```bash\n./applications/deltacast_transmitter/deltacast_transmitter --data <holohub_data_dir>/endoscopy\n```\n",
        "application_name": "deltacast_transmitter",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Basic Networking Ping",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk_version": "0.5.0",
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Networking",
                "Network",
                "UDP",
                "IP"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "basic_network",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/basic_networking_ping",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Basic Networking Ping\n\nThis application takes the existing ping example that runs over Holoscan ports and instead uses the basic\nnetwork operator to run over a UDP socket.\n\nThe basic network operator allows users to send and receive UDP messages over a standard Linux socket.\nSeparate transmit and receive operators are provided so they can run independently and better suit\nthe needs of the application.\n\n### Configuration\n\nThe application is configured using the file basic_networking_ping.yaml. Depending on how the machine\nis configured, the IP and UDP port likely need to be configured. All other settings do not need to be\nchanged.\n\nPlease refer to the basic network operator documentation for more configuration information\n\n### Requirements\n\nThis application requires:\n1. Linux\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n### Run Instructions\n\nFirst, go in your `build` or `install` directory. Then, run the commands of your choice:\n\n\n```bash\n./build/applications/basic_networking_ping/cpp/basic_networking_ping\n```\n",
        "application_name": "basic_networking_ping",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "H264 Endoscopy Tool Tracking",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Endoscopy",
                "Video Encoding"
            ],
            "ranking": 1,
            "dependencies": {
                "operators": [
                    {
                        "name": "videodecoder",
                        "version": "1.0"
                    },
                    {
                        "name": "videoencoder",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "./h264_endoscopy_tool_tracking h264_endoscopy_tool_tracking.yaml --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# H264 Endoscopy Tool Tracking Application\n\nThe application showcases how to use H.264 video source as input to and output\nfrom the Holoscan pipeline. This application is a modified version of Endoscopy\nTool Tracking reference application in Holoscan SDK that supports H.264\nelementary streams as the input and output.\n\n_The H.264 video decode operator does not adjust framerate as it reads the elementary\nstream input. As a result the video stream will be displayed as quickly as the decoding can be\nperformed. This feature will be coming soon to a new version of the operator._\n\n## Requirements\n\nThis application is configured to use H.264 elementary stream from endoscopy\nsample data as input. The output of the pipeline is again recorded to a H.264\nelementary stream on the disk, file name / path for this can be specified in\nthe 'h264_endoscopy_tool_tracking.yaml' file.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nUnzip the sample data:\n\n```\nunzip holoscan_endoscopy_sample_data_20230128.zip -d <data_dir>\n```\n\n## Building the application\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n## Running the application\n\nRun the application `h264_endoscopy_tool_tracking` in the binary directory.\n\n```bash\ncd <build_dir>/applications/h264_endoscopy_tool_tracking/ \\\n  && ./h264_endoscopy_tool_tracking --data <HOLOHUB_DATA_DIR>/endoscopy\n```\n",
        "application_name": "h264_endoscopy_tool_tracking",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Videomaster endoscopy example application",
            "authors": [
                {
                    "name": "Laurent Radoux",
                    "affiliation": "DELTACAST"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk_version": "0.5.0",
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DeltaCast",
                "VideoMaster",
                "Endoscopy",
                "Classification"
            ],
            "ranking": 3,
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data"
            },
            "run": {
                "command": "<holohub_app_bin>/deltacast_endoscopy_tool_tracking --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Deltacast Endoscopy Tool Tracking\n\nThis application application is based on the endoscopy_tool_tracking application and demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream using Deltacast's videomaster SDK.\n\n### Requirements\n\nThis application uses the DELTACAST.TV capture card for input stream. Contact [DELTACAST.TV](https://www.deltacast.tv/) for more details on how access the SDK and to setup your environment.\n\n### Data\n\nThis applications uses the dataset from the endoscopy tool tracking application:\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\n### Build Instructions\n\nSee instructions from the top level README on how to build this application.\nNote that this application requires to provide the VideoMaster_SDK_DIR if it is not located in a default location on the system.\nThis can be done with the following command, from the top level Holohub source directory:\n\n```bash\n./run build deltacast_endoscopy_tool_tracking --configure-args -DVideoMaster_SDK_DIR=<Path to VideoMasterSDK>\n```\n\n### Run Instructions\n\nFrom the build directory, run the command:\n\n```bash\n./applications/deltacast_endoscopy_tool_tracking/deltacast_endoscopy_tool_tracking --data <holohub_data_dir>/endoscopy\n```\n",
        "application_name": "deltacast_endoscopy_tool_tracking",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Endoscopy Tool Segmentation from MONAI Model Zoo Application",
            "authors": [
                {
                    "name": "Jin Lin",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk_version": "0.5.0",
            "platforms": [
                "amd64, arm64"
            ],
            "tags": [
                "MONAI",
                "Endoscopy",
                "Segmentation"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": []
            }
        },
        "readme": "# Endoscopic Tool Segmentation from MONAI Model Zoo\nThis endoscopy tool segmentation application runs the MONAI Endoscopic Tool Segmentation from [MONAI Model Zoo](https://github.com/Project-MONAI/model-zoo/tree/dev/models/endoscopic_tool_segmentation).\n\n\nThis HoloHub application has been verified on the GI Genius sandbox and is currently deployable to GI Genius Intelligent Endoscopy Modules. [GI Genius](https://www.cosmoimd.com/gi-genius/) is Cosmo Intelligent Medical Devices\u2019 AI-powered endoscopy system. This implementation by Cosmo Intelligent Medical Devices showcases the fast and seamless deployment of HoloHub applications on products/platforms running on NVIDIA Holoscan.\n\n## Model\nWe will be deploying the endoscopic tool segmentation model from [MONAI Model Zoo](https://github.com/Project-MONAI/model-zoo/tree/dev/models/endoscopic_tool_segmentation). <br>\nNote that you could also use the MONAI model zoo repo for training your own semantic segmentation model with your own data, but here we are directly deploying the downloaded MONAI model checkpoint into Holoscan. \n\n\n### Model conversion to ONNX\nBefore deploying the MONAI Model Zoo's trained model checkpoint in Holoscan SDK, we convert the model checkpoint into ONNX. <br>\nYou can choose to \n- download the [already converted ONNX model here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/monai_endoscopic_tool_segmentation_model) directly and skip the rest of this Model section, or \n- go through the following conversion steps yourself. \n\n 1. Download the PyTorch model checkpoint linked in the README of [endoscopic tool segmentation](https://github.com/Project-MONAI/model-zoo/tree/dev/models/endoscopic_tool_segmentation#model-overview). We will assume its name to be `model.pt`.\n 2. Clone the MONAI Model Zoo repo. \n```\ncd [your-workspace]\ngit clone https://github.com/Project-MONAI/model-zoo.git\n```\nand place the downloaded PyTorch model into `model-zoo/models/endoscopic_tool_segmentation/`.\n\n 3. Pull and run the docker image for [MONAI](https://hub.docker.com/r/projectmonai/monai). We will use this docker image for converting the PyTorch model to ONNX. \n```\ndocker pull projectmonai/monai\ndocker run -it --rm --gpus all -v [your-workspace]/model-zoo:/workspace/model-zoo -w /workspace/model-zoo/models/endoscopic_tool_segmentation/ projectmonai/monai\n```\n 4. Install onnxruntime within the container\n ```\npip install onnxruntime onnx-graphsurgeon\n ```\n 5. Convert model\n \nWe will first export the model.pt file to ONNX by using the [export_to_onnx.py](https://github.com/Project-MONAI/model-zoo/blob/dev/models/endoscopic_tool_segmentation/scripts/export_to_onnx.py) file. Modify the backbone in [line 122](https://github.com/Project-MONAI/model-zoo/blob/dev/models/endoscopic_tool_segmentation/scripts/export_to_onnx.py#L122) to be efficientnet-b2:\n```\nmodel = load_model_and_export(modelname, outname, out_channels, height, width, multigpu, backbone=\"efficientnet-b2\")\n```\nNote that the model in the Model Zoo here was trained to have only two output channels: label 1 = tools, label 0 = everything else, but the same Model Zoo repo can be repurposed to train a model with a different dataset that has more than two classes.\n```\npython scripts/export_to_onnx.py --model model.pt --outpath model_endoscopic_tool_seg.onnx --width 736 --height 480 --out_channels 2\n```\nFold constants in the ONNX model.\n```\npolygraphy surgeon sanitize --fold-constants model_endoscopic_tool_seg.onnx -o model_endoscopic_tool_seg_sanitized.onnx\n```\nFinally, modify the input and output channels to have shape [n, height, width, channels], [n, channels, height, width]. \n```\npython scripts/graph_surgeon_tool_seg.py --orig_model model_endoscopic_tool_seg_sanitized.onnx --new_model model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx\n```\n\n## Data\nFor this application we will use the same [Endoscopy Sample Data](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data) as the Holoscan SDK reference applications.\n\n## Requirements\nThe only requirement is to make sure the model and data are accessible by the application. If running inside a Docker container, please mount the path(s) where the model and data are.\n## Running the application\nIf running within a Docker container, while launching the container, make sure this directory containing applications and the directory containing models are mounted in the runtime container. Add the `-v` mount options to the `docker run` command.\n\nMake sure the [yaml file](./tool_segmentation.yaml) is pointing to the right locations for the ONNX model and data. The assumption in the yaml file is that the converted ONNX model is located at:\n```\nmodel_file_path: /byom/models/tool_seg/model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx\nengine_cache_dir: /byom/models/tool_seg/model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out_engines\n```\nand the [Endoscopy Sample Data](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data) is assumed to be at \n```\n/workspace/holoscan-sdk/data/endoscopy\n```\n\nPlease check and modify the paths to model and data in the yaml file if needed.\n\nRun the Python application simply by:\n```\npython tool_segmentation.py\n```\n",
        "application_name": "monai_endoscopic_tool_seg",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Simple Classical Radar Pipeline",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk_version": "0.4.0",
            "platforms": [
                "amd64"
            ],
            "tags": [
                "Aerospace, Defense, Communications"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "numpy",
                        "version": "1.23.2"
                    },
                    {
                        "name": "cupy",
                        "version": "11.4"
                    },
                    {
                        "name": "cusignal",
                        "version": "22.12"
                    }
                ]
            },
            "run": {
                "command": "python3 simple_radar_pipeline.py",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# Simple Radar Pipeline Application\n\nThis demonstration walks the developer through building a simple radar signal processing pipeline, targeted towards detecting objects, with Holoscan. In this example, we generate random radar and waveform data, passing both through:\n1. Pulse Compression\n2. Moving Target Indication (MTI) Filtering\n3. Range-Doppler Map\n4. Constant False Alarm Rate (CFAR) Analysis\n\nWhile this example generates 'offline' complex-valued data, it could be extended to accept streaming data from a phased array system or simulation via modification of the `SignalGeneratorOperator`.\n\nThe output of this demonstration is a measure of the number of pulses per second processed on GPU.\n\n The main objectives of this demonstration are to:\n- Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries\n- Demonstrate how to construct and connect isolated units of work via Holoscan operators, particularly with handling multiple inputs and outputs into an Operator\n- Emphasize that operators created for this application can be re-used in other ones doing similar tasks\n\n# Running the Application\n\nPrior to running the application, the user needs to install the necessary dependencies. This is most easily done in an Anaconda environment.\n\n```\nconda create --name holoscan-sdr-demo python=3.8\nconda activate holoscan-sdr-demo\nconda install -c conda-forge -c rapidsai -c nvidia cusignal\npip install holoscan\n```\n\nThe simple radar signal processing pipeline example can then be run via\n```\npython applications/simple_radar_pipeline/simple_radar_pipeline.py\n```\n",
        "application_name": "simple_radar_pipeline",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Simple Radar Pipeline in C++",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk_version": "0.4.0",
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Signal Processing",
                "RADAR"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "MatX",
                        "version": "0.2.5",
                        "url": "https://github.com/NVIDIA/MatX.git"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/simple_radar_pipeline",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Simple Radar Pipeline Application\n\nThis demonstration walks the developer through building a simple radar signal processing pipeline, targeted towards detecting objects, with Holoscan. In this example, we generate random radar and waveform data, passing both through:\n1. Pulse Compression\n2. Moving Target Indication (MTI) Filtering\n3. Range-Doppler Map\n4. Constant False Alarm Rate (CFAR) Analysis\n\nWhile this example generates 'offline' complex-valued data, it could be extended to accept streaming data from a phased array system or simulation via modification of the `SignalGeneratorOperator`.\n\nThe output of this demonstration is a measure of the number of pulses per second processed on GPU.\n\n The main objectives of this demonstration are to:\n- Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries\n- Demonstrate how to construct and connect isolated units of work via Holoscan operators, particularly with handling multiple inputs and outputs into an Operator\n- Emphasize that operators created for this application can be re-used in other ones doing similar tasks\n\n## Building the application\nMake sure CMake (https://www.cmake.org) is installed on your system (minimum version 3.20)\n\n- [Holoscan Debian Package](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_dev_deb) - Follow the instructions in the link to install the latest version of Holoscan Debian package from NGC.\n\n- Create a build directory:\n  ```bash\n  mkdir -p <build_dir> && cd <build_dir>\n  ```\n- Configure with CMake:\n\n  Make sure CMake can find your installation of the Holoscan SDK. For example, setting `holoscan_ROOT` to its install directory during configuration:\n\n  ```bash\n  cmake -S <source_dir> -B <build_dir> -DAPP_simple_radar_pipeline=1 \n  ```\n\n  _Notes:_\n  _If the error `No CMAKE_CUDA_COMPILER could be found` is encountered, make sure that the :code:`nvcc` executable can be found by adding the CUDA runtime location to your `PATH` variable:_\n\n  ```\n  export PATH=$PATH:/usr/local/cuda/bin\n  ```\n\n- Build:\n\n  ```bash\n  cmake --build <build_dir>\n  ```\n\n## Running the application\n```bash\n<build_dir>/simple_radar_pipeline\n```\n\n",
        "application_name": "simple_radar_pipeline",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Power Spectral Density with cuNumeric",
            "authors": [
                {
                    "name": "Adam Thompson",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64"
            ],
            "tags": [
                "Life Sciences, Aerospace, Defense, Communications"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "numpy",
                        "version": "1.24.2"
                    },
                    {
                        "name": "cupy",
                        "version": "12.0"
                    },
                    {
                        "name": "cunumeric",
                        "version": "23.03"
                    }
                ]
            }
        },
        "readme": "# Calculate Power Spectral Density with Holoscan and cuNumeric\n\n[cuNumeric](https://github.com/nv-legate/cunumeric) is an drop-in replacement for NumPy that aims to provide a distributed and accelerated drop-in replacement for the NumPy API on top of the [Legion](https://legion.stanford.edu/) runtime. It works best for programs that have very large arrays of data that can't fit in the the memory of a single GPU or node.\n\nIn this example application, we are using the cuNumeric library within a Holoscan application graph to determine the Power Spectral Density (PSD) of an incoming signal waveform. Notably, this is simply achieved by taking the absolute value of the FFT of a data array.\n\n The main objectives of this demonstration are to:\n- Highlight developer productivity in building an end-to-end streaming application with Holoscan and cuNumeric\n- Demonstrate how to scale a given workload to multiple GPUs using cuNumeric\n\n# Running the Application\n\nPrior to running the application, the user needs to install the necessary dependencies. This is most easily done in an Anaconda environment.\n\n```\nconda create --name holoscan-cunumeric-demo python=3.9\nconda activate holoscan-cunumeric-demo\nconda install -c nvidia -c conda-forge -c legate cunumeric cupy\npip install holoscan\n```\n\nThe cuNumeric PSD processing pipeline example can then be run via\n```\nlegate --gpus 2 applications/cunumeric_integration/cunumeric_psd.py\n```\n\nWhile running the application, you can confirm multi GPU utilization via watching `nvidia-smi` or using another GPU utilization tool\n\nTo run the same application without cuNumeric, simply change `import cunumeric as np` to `import cupy as np` in the code and run\n```\npython applications/cunumeric_integration/cunumeric_psd.py\n```\n",
        "application_name": "cunumeric_integration",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "USB/HDMI Video Capture",
            "authors": [
                {
                    "name": "Mikael Brudfors",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Camera"
            ],
            "ranking": 2,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "v4l2_video_capture",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/usb_hdmi_video_capture.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Capture USB and HDMI Video Stream\n\nThis app captures either a USB or an HDMI video stream and visualizes it using Holoviz.\n\n## Requirements\n\nInstall the following dependency:\n```sh\nsudo apt-get install libv4l-dev=1.18.0-2build1\n```\n\nNote that you might not have permissions to open the video device(s), run:\n```sh\n sudo chmod 666 /dev/video?\n ```\n to make them available.\n\n## Parameters\n\nMake sure that the `pixel_format` parameter in the YAML config file is set correctly, options are `RGBA32` and `YUYV`. Also make sure that the `device` parameter is set to the mount point of the device you want to stream from. \n\nThese parameters can be found with:\n```sh\nv4l2-ctl --list-devices\n```\nfollowed by:\n```sh\nv4l2-ctl -d /dev/video0 --list-formats-ext\n```\nIf you do not have the `v4l2-ctl` app, it can be installed with:\n```sh\nsudo apt-get install v4l-utils\n```\n\n## HDMI IN\n\nThe HDMI IN on the dev kit will have to be activated in order for capturing to work. Please look at the relevant dev kit user guide for instructions. Additionally, the parameter `pixel_format` needs to be set to `RGBA32` and  the `device` parameter set to the mount point of the HDMI IN device.\n\n## Run Instructions\n\nFirst, build the app with the root folder `run.sh` script:\n```sh\n./run build usb_hdmi_video_capture\n```\n\n### C++\n\n```sh\ncd build/applications/usb_hdmi_video_capture/cpp\n./usb_hdmi_video_capture\n```\n\n### Python\n\n```sh\ncd applications/usb_hdmi_video_capture/python/\npython3 usb_hdmi_video_capture.py\n```",
        "application_name": "usb_hdmi_video_capture",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "USB/HDMI Video Capture",
            "authors": [
                {
                    "name": "Mikael Brudfors",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Camera"
            ],
            "ranking": 2,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "v4l2_video_capture",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/usb_hdmi_video_capture",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Capture USB and HDMI Video Stream\n\nThis app captures either a USB or an HDMI video stream and visualizes it using Holoviz.\n\n## Requirements\n\nInstall the following dependency:\n```sh\nsudo apt-get install libv4l-dev=1.18.0-2build1\n```\n\nNote that you might not have permissions to open the video device(s), run:\n```sh\n sudo chmod 666 /dev/video?\n ```\n to make them available.\n\n## Parameters\n\nMake sure that the `pixel_format` parameter in the YAML config file is set correctly, options are `RGBA32` and `YUYV`. Also make sure that the `device` parameter is set to the mount point of the device you want to stream from. \n\nThese parameters can be found with:\n```sh\nv4l2-ctl --list-devices\n```\nfollowed by:\n```sh\nv4l2-ctl -d /dev/video0 --list-formats-ext\n```\nIf you do not have the `v4l2-ctl` app, it can be installed with:\n```sh\nsudo apt-get install v4l-utils\n```\n\n## HDMI IN\n\nThe HDMI IN on the dev kit will have to be activated in order for capturing to work. Please look at the relevant dev kit user guide for instructions. Additionally, the parameter `pixel_format` needs to be set to `RGBA32` and  the `device` parameter set to the mount point of the HDMI IN device.\n\n## Run Instructions\n\nFirst, build the app with the root folder `run.sh` script:\n```sh\n./run build usb_hdmi_video_capture\n```\n\n### C++\n\n```sh\ncd build/applications/usb_hdmi_video_capture/cpp\n./usb_hdmi_video_capture\n```\n\n### Python\n\n```sh\ncd applications/usb_hdmi_video_capture/python/\npython3 usb_hdmi_video_capture.py\n```",
        "application_name": "usb_hdmi_video_capture",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "MultiAI Ultrasound",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Ultrasound",
                "MultiAI"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "python3 <holohub_app_source>/multiai_ultrasound.py  --data <holohub_data_dir>/multiai_ultrasound",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Multi-AI Ultrasound\n\nThis application demonstrates how to run multiple inference pipelines in a single application by leveraging the Holoscan Inference module, a framework that facilitates designing and executing inference applications in the Holoscan SDK.\n\nThe Multi AI operators (inference and postprocessor) use APIs from the Holoscan Inference module to extract data, initialize and execute the inference workflow, process, and transmit data for visualization.\n\nThe applications uses models and echocardiogram data from iCardio.ai. The models include:\n- a Plax chamber model, that identifies four critical linear measurements of the heart\n- a Viewpoint Classifier model, that determines confidence of each frame to known 28 cardiac anatomical view as defined by the guidelines of the American Society of Echocardiography\n- an Aortic Stenosis Classification model, that provides a score which determines likeability for the presence of aortic stenosis\n\nThe default configuration (`multiai_ultrasound.yaml`) runs on default GPU (GPU-0). Multi-AI Ultrasound application can be executed on multiple GPUs with the Holoscan SDK version 0.6 onwards. A sample configuration file for multi GPU configuration for multi-AI ultrasound application (`mgpu_multiai_ultrasound.yaml`) is present in both `cpp` and `python` applications. The multi-GPU configuration file is designed for a system with at least 2 GPUs connected to the same PCIE network.\n\n### Requirements\n\n- Python 3.8+\n- The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the echocardiogram (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for Multi-AI Ultrasound Pipeline](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_multi_ai_ultrasound_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\nNext, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/multiai_ultrasound/python\n    python3 multiai_ultrasound.py --source=replayer --data <DATA_DIR>/multiai_ultrasound\n    ```\n\n* Using a pre-recorded video on multi-GPU system\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/multiai_ultrasound/python\n    python3 multiai_ultrasound.py --config mgpu_multiai_ultrasound.yaml --source=replayer --data <DATA_DIR>/multiai_ultrasound\n    ```\n\n* Using an AJA card\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/multiai_ultrasound/python\n    python3 multiai_ultrasound.py --source=aja\n    ```\n",
        "application_name": "multiai_ultrasound",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "MultiAI Ultrasound",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Ultrasound",
                "MultiAI"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "<holohub_app_bin>/multiai_ultrasound --data <holohub_data_dir>/multiai_ultrasound",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Multi-AI Ultrasound\n\nThis application demonstrates how to run multiple inference pipelines in a single application by leveraging the Holoscan Inference module, a framework that facilitates designing and executing inference applications in the Holoscan SDK.\n\nThe Inference and the Processing operators use APIs from the Holoscan Inference module to extract data, initialize and execute the inference workflow, process, and transmit data for visualization.\n\nThe applications uses models and echocardiogram data from iCardio.ai. The models include:\n- a Plax chamber model, that identifies four critical linear measurements of the heart\n- a Viewpoint Classifier model, that determines confidence of each frame to known 28 cardiac anatomical view as defined by the guidelines of the American Society of Echocardiography\n- an Aortic Stenosis Classification model, that provides a score which determines likeability for the presence of aortic stenosis\n\nThe default configuration (`multiai_ultrasound.yaml`) runs on default GPU (GPU-0). Multi-AI Ultrasound application can be executed on multiple GPUs with the Holoscan SDK version 0.6 onwards. A sample configuration file for multi GPU configuration for multi-AI ultrasound application (`mgpu_multiai_ultrasound.yaml`) is present in both `cpp` and `python` applications. The multi-GPU configuration file is designed for a system with at least 2 GPUs connected to the same PCIE network.\n\n### Requirements\n\nThe provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the echocardiogram (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for Multi-AI Ultrasound Pipeline](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_multi_ai_ultrasound_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n### Run Instructions\n\nIn your `build` directory, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    sed -i -e 's#^source:.*#source: replayer#' applications/multiai_ultrasound/cpp/multiai_ultrasound.yaml\n    applications/multiai_ultrasound/cpp/multiai_ultrasound --data <DATA_DIR>/multiai_ultrasound\n    ```\n\n* Using a pre-recorded video on multi-GPU system\n    ```bash\n    sed -i -e 's#^source:.*#source: replayer#' applications/multiai_ultrasound/cpp/mgpu_multiai_ultrasound.yaml\n    applications/multiai_ultrasound/cpp/multiai_ultrasound applications/multiai_ultrasound/cpp/mgpu_multiai_ultrasound.yaml --data <DATA_DIR>/multiai_ultrasound\n    ```\n\n* Using an AJA card\n    ```bash\n    sed -i -e 's#^source:.*#source: aja#' applications/multiai_ultrasound/cpp/multiai_ultrasound.yaml\n    applications/multiai_ultrasound/cpp/multiai_ultrasound\n    ```\n",
        "application_name": "multiai_ultrasound",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Endoscopy Out of Body Detection Pipeline in C++",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Endoscopy",
                "Classification"
            ],
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/endoscopy_out_of_body_detection",
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/endoscopy_out_of_body_detection"
            },
            "run": {
                "command": "./endoscopy_out_of_body_detection endoscopy_out_of_body_detection.yaml --data <holohub_data_dir>/endoscopy_out_of_body_detection",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Endoscopy Out of Body Detection Application\n\nThis application performs endoscopy out of body detection. The application classifies if the input frame is inside the body or out of the body. If the input frame is inside the body, application prints `Likely in-body`, otherwise `Likely out-of-body`. Each likelihood is accompanied with a confidence score. \n\n__Note: there is no visualization component in the application.__\n\n`endoscopy_out_of_body_detection.yaml` is the configuration file. Input video file is converted into GXF tensors and the name and location of the GXF tensors are updated in the `basename` and the `directory` field in `replayer`.\n\n## Data\n\n__Note: the data is automatically downloaded and converted when building. If you need to manually convert the data follow the following steps.__\n\n\n* Endoscopy out of body detection model and the sample dataset is available on [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/endoscopy_out_of_body_detection)\n  After downloading the data in mp4 format, it must be converted into GXF tensors.\n* Script for GXF tensor conversion (`convert_video_to_gxf_entities.py`) is available with the Holoscan SDK, and can be accessed [here](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts)\n\n### Unzip and convert the sample data:\n\n```\n# unzip the downloaded sample data\nunzip [FILE].zip -d <data_dir>\n\n# convert video file into tensor\nffmpeg -i <INPUT_VIDEO_FILE> -fs 900M -pix_fmt rgb24 -f rawvideo pipe:1 | python convert_video_to_gxf_entities.py --width 256 --height 256 --channels 3 --framerate 30\n\n# where <INPUT_VIDEO_FILE> is one of the downloaded MP4 files: OP1-out-2.mp4, OP4-out-8.mp4 or OP8-out-4.mp4.\n```\n\nMove the model file and converted video tensor into a directory structure similar to the following:\n\n```bash\ndata\n\u2514\u2500\u2500 endoscopy_out_of_body_detection\n    \u251c\u2500\u2500 LICENSE.md\n    \u251c\u2500\u2500 out_of_body_detection.onnx\n    \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_entities\n    \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_index\n    \u2514\u2500\u2500 sample_clip_out_of_body_detection.mp4\n```\n\n## Building the application\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n## Running the application\n\nIn your `build` directory, run\n\n```bash\napplications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection --data ../data/endoscopy_out_of_body_detection\n```\n",
        "application_name": "endoscopy_out_of_body_detection",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Speech-to-text + Large Language Model",
            "authors": [
                {
                    "name": "Sean Huver",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64"
            ],
            "tags": [
                "Speech-to-text",
                "Large Language Model"
            ],
            "ranking": 2,
            "dependencies": {
                "openai-whisper": "^20230314",
                "openai": "^0.27.2"
            },
            "run": {
                "command": "python3 stt_to_nlp.py",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# HoloHub Applications\n\nThis directory contains applications based on the Holoscan Platform.\nSome applications might require specific hardware and software packages which are described in the \nmetadata.json and/or README.md for each application.\n\n# Contributing to HoloHub Applications\n\nPlease review the [CONTRIBUTING.md file](https://github.com/nvidia-holoscan/holohub/blob/main/CONTRIBUTING.md) guidelines to contribute applications.\n",
        "application_name": "speech_to_text_llm",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "SSD Detection Application",
            "authors": [
                {
                    "name": "Jin Lin",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk_version": "0.5.0",
            "platforms": [
                "amd64, arm64"
            ],
            "tags": [
                "SSD",
                "bounding box",
                "Detection"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "numpy",
                        "version": "1.22.3"
                    },
                    {
                        "name": "cupy",
                        "version": "11.3.0"
                    }
                ]
            }
        },
        "readme": "# SSD Detection Application\n## Model\nWe can train the [SSD model from NVIDIA DeepLearningExamples repo]((https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD)) with any data of our choosing. Here for the purpose of demonstrating the deployment process, we will use a SSD model checkpoint that is only trained for the [demo video clip](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data). \n\nPlease download the models [at this NGC Resource](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/ssd_surgical_tool_detection_model) for `epoch_24.pt`, `epoch24_nms.onnx` and `epoch24.onnx`. You can go through the next steps of Model Conversion to ONNX to convert `epoch_24.pt` into `epoch24_nms.onnx` and `epoch24.onnx`, or use the downloaded ONNX models directly.\n\n\n### Model Conversion to ONNX\nThe scripts we need to export the model from .pt checkpoint to the ONNX format are all within this dir `./scripts`. It is a two step process.\n\n\n Step 1: Export the trained checkpoint to ONNX. <br> We use [`export_to_onnx_ssd.py`](./scripts/export_to_onnx_ssd.py) if we want to use the model as is without NMS, or [`export_to_onnx_ssd_nms.py`](./scripts/export_to_onnx_ssd_nms.py) to prepare the model with NMS. \n Let's assume the re-trained SSD model checkpoint from the [repo](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD) is saved as `epoch_24.pt`.\n The export process is \n```\n# For exporting the original ONNX model\n python export_to_onnx_ssd.py --model epoch_24.pt  --outpath epoch24_temp.onnx\n```\n```\n# For preparing to add the NMS step to ONNX model\npython export_to_onnx_ssd_nms.py --model epoch_24.pt  --outpath epoch24_nms_temp.onnx\n```\nStep 2: modify input shape. <br> Step 1 produces a onnx model with input shape `[1, 3, 300, 300]`, but we will want to modify the input node to have shape `[1, 300, 300, 3]` or in general `[batch_size, height, width, channels]` for compatibility and easy of deployment in the Holoscan SDK. If we want to incorporate the NMS operation in the the ONNX model, we could add a `EfficientNMS_TRT` op, which is documented in [`graph_surgeon_ssd.py`](./scripts/graph_surgeon_ssd.py)'s nms related block.\n```\n# For exporting the original ONNX model\npython graph_surgeon_ssd.py --orig_model epoch24_temp.onnx --new_model epoch24.onnx\n```\n```\n# For adding the NMS step to ONNX model, use --nms\npython graph_surgeon_ssd.py --orig_model epoch24_nms_temp.onnx --new_model epoch24_nms.onnx --nms\n```\n\nNote that\n - `epoch24.onnx` is used in `ssd_step1.py` and `ssd_step2_route1.py`\n - `epoch24_nms.onnx` is used in `ssd_step2_route2.py` and `ssd_step2_route2_render_labels.py` \n\n## Data\nFor this application we will use the same [Endoscopy Sample Data](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data) as the Holoscan SDK reference applications.\n\n## Requirements\nThere are two requirements \n1. To run `ssd_step1.py` and `ssd_step2_route1.py` with the original exported model, we need the installation of PyTorch and CuPy. <br> To run `ssd_step2_route2.py` and `ssd_step2_route2_render_labels.py` with the exported model with additional NMS layer in ONNX, we need the installation of CuPy. <br>If you choose to build the SDK from source, you can find the [modified Dockerfile here](./docker/Dockerfile) to replace the SDK repo [Dockerfile](https://github.com/nvidia-holoscan/holoscan-sdk/blob/main/Dockerfile) to satisfy the installation requirements. \n<br>\nThe main changes in Dockerfile: \n<br>(1) the base image changed to `nvcr.io/nvidia/pytorch:22.03-py3` instead of the `nvcr.io/nvidia/tensorrt:22.03-py3`. This is mainly for the PyTorch dependency in `ssd_step1.py` and `ssd_step2_route1.py` while running on aarch64 developer kits, as there are no compatible PyTorch pip wheels available for aarch64. <br>(2) added the installation of NVTX for profiling.\n<br>Then, build the dev container following the [README instructions](https://github.com/nvidia-holoscan/holoscan-sdk#recommended-using-the-run-script).<br>\nWhen launching the container, make sure this directory containing applications and the directory containing models are mounted in the runtime container. Add the `-v` mount options to the `docker run` command.\n\n2. Make sure the model and data are accessible by the application. \n<br> Make sure the yaml file ssd_endo_model.yaml and ssd_endo_model_with_NMS.yaml are pointing to the right locations for the ONNX model and data. The assumption in the yaml file is that the `epoch24_nms.onnx` and `epoch24.onnx` are located at:\n```\nmodel_file_path: /byom/models/endo_ssd/epoch24_nms.onnx \nengine_cache_dir: /byom/models/endo_ssd/epoch24_nms_engines\n```\nand / or\n```\nmodel_file_path: /byom/models/endo_ssd/epoch24.onnx\nengine_cache_dir: /byom/models/endo_ssd/epoch24_engines\n```\nThe [Endoscopy Sample Data](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data) is assumed to be at \n```\n/workspace/holoscan-sdk/data/endoscopy\n```\nPlease check and modify the paths to model and data in the yaml file if needed.\n\n## Building the application\nPlease refer to the README under [./app_dev_process](./app_dev_process/README.md) to see the process of building the applications.\n\n## Running the application\nRun the incrementally improved Python applications by:\n```\npython ssd_step1.py\n\npython ssd_step2_route1.py\n\npython ssd_step2_route2.py\n\npython ssd_step2_route2_render_labels.py --labelfile endo_ref_data_labels.csv\n```",
        "application_name": "ssd_detection_endoscopy_tools",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Ultrasound Segmentation",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Ultrasound",
                "Segmentation"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "python3 <holohub_app_source>/ultrasound_segmentation.py  --data <holohub_data_dir>/ultrasound_segmentation",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Ultrasound Bone Scoliosis Segmentation\n\nFull workflow including a generic visualization of segmentation results from a spinal scoliosis segmentation model of ultrasound videos. The model used is stateless, so this workflow could be configured to adapt to any vanilla DNN model. \n\n### Requirements\n\n- Python 3.8+\n- The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the ultrasound data (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Bone Scoliosis Segmentation](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_ultrasound_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\nNext, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/ultrasound_segmentation/python\n    python3 ultrasound_segmentation.py --source=replayer --data <DATA_DIR>/ultrasound_segmentation\n    ```\n\n* Using an AJA card\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/ultrasound_segmentation/python\n    python3 ultrasound_segmentation.py --source=aja\n    ```\n",
        "application_name": "ultrasound_segmentation",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Ultrasound Segmentation",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Ultrasound",
                "Segmentation"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "<holohub_app_bin>/ultrasound_segmentation  --data <holohub_data_dir>/ultrasound_segmentation",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Ultrasound Bone Scoliosis Segmentation\n\nFull workflow including a generic visualization of segmentation results from a spinal scoliosis segmentation model of ultrasound videos. The model used is stateless, so this workflow could be configured to adapt to any vanilla DNN model. \n\n### Requirements\n\nThe provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the ultrasound data (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Bone Scoliosis Segmentation](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_ultrasound_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n\n### Run Instructions\n\nIn your `build` directory, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    sed -i -e 's#^source:.*#source: replayer#' applications/ultrasound_segmentation/cpp/ultrasound_segmentation.yaml\n    applications/ultrasound_segmentation/cpp/ultrasound_segmentation --data <data_dir>/ultrasound_segmentation\n    ```\n\n* Using an AJA card\n    ```bash\n    sed -i -e 's#^source:.*#source: aja#' applications/ultrasound_segmentation/cpp/ultrasound_segmentation.yaml\n    applications/ultrasound_segmentation/cpp/ultrasound_segmentation\n    ```\n",
        "application_name": "ultrasound_segmentation",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "H.264 Video Decode Reference Application",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "H.264",
                "Video Decoding"
            ],
            "ranking": 1,
            "dependencies": {
                "operators": [
                    {
                        "name": "videodecoder",
                        "version": "1.0"
                    },
                    {
                        "name": "videodecoderio",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "./h264_video_decode h264_video_decode.yaml --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# H.264 Video Decode Reference Application\n\nThis is a minimal reference application demonstrating usage of H.264 video\ndecode operator. This application makes use of H.264 elementary stream reader\noperator for reading H.264 elementary stream input and uses Holoviz operator\nfor rendering decoded data to the native window.\n\n_The H.264 video decode operator does not adjust framerate as it reads the elementary\nstream input. As a result the video stream will be displayed as quickly as the decoding can be\nperformed. This feature will be coming soon to a new version of the operator._\n\n## Requirements\n\nThis application is configured to use H.264 elementary stream from endoscopy\nsample data as input. To use any other stream, the filename / path for the\ninput file can be specified in the 'h264_video_decode.yaml' file.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nUnzip the sample data:\n\n```\nunzip holoscan_endoscopy_sample_data_20230128.zip -d <data_dir>\n```\n\n## Building the application\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n## Running the application\n\nRun the application `h264_video_decode` in the binary directory.\n\n```bash\ncd <build_dir>/applications/h264_video_decode && ./h264_video_decode --data <HOLOHUB_DATA_DIR>/endoscopy\n```\n",
        "application_name": "h264_video_decode",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "High Speed Endoscopy",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Camera",
                "Emergent"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "emergent_source",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/high_speed_endoscopy.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# High-Speed Endoscopy\n\nThe application showcases how high resolution cameras can be used to capture the scene, post-processed on GPU and displayed at high frame rate.\n\n### Requirements\n\nThis application requires:\n1. an Emergent Vision Technologies camera (see [setup instructions]((https://docs.nvidia.com/clara-holoscan/sdk-user-guide/emergent_setup.html)\n2. a NVIDIA ConnectX SmartNIC with Rivermax SDK and drivers installed (see [prerequisites](../../README.md#prerequisites))\n3. a display with high refresh rate to keep up with the camera's framerate\n4. [additional setups](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/additional_setup.html) to reduce latency\n\n### Run Instructions\n\nTODO\n",
        "application_name": "high_speed_endoscopy",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "High Speed Endoscopy",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Camera",
                "Emergent"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "emergent_source",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/high_speed_endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# High-Speed Endoscopy\n\nThe application showcases how high resolution cameras can be used to capture the scene, post-processed on GPU and displayed at high frame rate.\n\n### Requirements\n\nThis application requires:\n1. an Emergent Vision Technologies camera (see [setup instructions]((https://docs.nvidia.com/clara-holoscan/sdk-user-guide/emergent_setup.html)\n2. a NVIDIA ConnectX SmartNIC with Rivermax SDK and drivers installed (see [prerequisites](../../README.md#prerequisites))\n3. a display with high refresh rate to keep up with the camera's framerate\n4. [additional setups](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/additional_setup.html) to reduce latency\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n> \u26a0\ufe0f At this time, camera controls are hardcoded within the `gxf_emergent_source` extension. To update them at the application level, the GXF extension, and the application need to be rebuilt.\nFor more information on the controls, refer to the [EVT Camera Attributes Manual](https://emergentvisiontec.com/resources/?tab=umg)\n\n### Run Instructions\n\nFirst, go in your `build` or `install` directory. Then, run the commands of your choice:\n\n* RDMA disabled\n    ```bash\n    # C++\n    sed -i -e 's#rdma:.*#rdma: false#' ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy.yaml \\\n        && sudo ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy\n    ```\n\n* RDMA enabled\n    ```bash\n    # C++\n    sed -i -e 's#rdma:.*#rdma: true#' ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy.yaml \\\n        && sudo MELLANOX_RINGBUFF_FACTOR=14 ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy\n    ```\n\n> \u2139\ufe0f The `MELLANOX_RINGBUFF_FACTOR` is used by the EVT driver to decide how much BAR1 size memory would be used on the dGPU. It can be changed to different number based on different use cases.\n",
        "application_name": "high_speed_endoscopy",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Endoscopy Tool Tracking",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Endoscopy",
                "Tracking",
                "AJA"
            ],
            "ranking": 1,
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data"
            },
            "run": {
                "command": "python3 <holohub_app_source>/endoscopy_tool_tracking.py --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Endoscopy Tool Tracking\n\nBased on a LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.\n\n### Requirements\n\n- Python 3.8+\n- The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded endoscopy video (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n \n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n \nNext, run the commands of your choice:\n\nThis application should **be run in the build directory of Holohub** in order to load the GXF extensions.\nAlternatively, the relative path of the extensions in the corresponding yaml file can be modified to match path of\nthe working directory.\n\n* Using a pre-recorded video\n    ```bash\n    cd <HOLOHUB_BUILD_DIR>\n    python3 <HOLOHUB_SOURCE_DIR>/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=replayer --data=<DATA_DIR>/endoscopy\n    ```\n\n* Using an AJA card\n    ```bash\n    cd <HOLOHUB_BUILD_DIR>\n    python3  <HOLOHUB_SOURCE_DIR>/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=aja\n    ```\n",
        "application_name": "endoscopy_tool_tracking",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Endoscopy Tool Tracking",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Endoscopy",
                "Tracking",
                "AJA"
            ],
            "ranking": 1,
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data"
            },
            "run": {
                "command": "<holohub_app_bin>/endoscopy_tool_tracking --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Endoscopy Tool Tracking\n\nBased on a LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.\n\n### Requirements\n\nThe provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded endoscopy video (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n### Run Instructions\n\nIn your `build` directory, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    sed -i -e 's#^source:.*#source: replayer#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data <data_dir>/endoscopy\n    ```\n\n* Using an AJA card\n    ```bash\n    sed -i -e 's#^source:.*#source: aja#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n    ```\n",
        "application_name": "endoscopy_tool_tracking",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Colonoscopy segmentation",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Colonoscopy",
                "Classification"
            ],
            "dependencies": {
                "data": "https://ngc.nvidia.com/resources/colonoscopy_sample_app_data"
            },
            "run": {
                "command": "python3 colonoscopy_segmentation.py --data <holohub_data_dir>/colonoscopy_segmentation",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# Colonoscopy Polyp Segmentation\n\nFull workflow including a generic visualization of segmentation results from a polyp segmentation models.\n\n### Requirements\n\n- Python 3.8+\n- The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the ultrasound data (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI Colonoscopy Segmentation of Polyps](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_colonoscopy_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\nNext, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/colonoscopy_segmentation\n    python3 colonoscopy_segmentation.py --source=replayer --data=<DATA_DIR>/colonoscopy_segmentation\n    ```\n\n* Using an AJA card\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/colonoscopy_segmentation\n    python3 colonoscopy_segmentation.py --source=aja\n    ```\n\n### Holoscan SDK version\n\nColonoscopy segmentation application in HoloHub requires version 0.6+ of the Holoscan SDK.\nIf the Holoscan SDK version is 0.5 or lower, following code changes must be made in the application:\n\n* In python/CMakeLists.txt: update the holoscan SDK version from `0.6` to `0.5`\n* In python/multiai_ultrasound.py: `InferenceOp` is replaced with `MultiAIInferenceOp`\n",
        "application_name": "colonoscopy_segmentation",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "video_read_bitstream",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "video"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### Video Bit Stream Reader\n\nThe `video_read_bitstream` reads h264 bit stream from specified input file.\n\n#### `holoscan::ops::VideoReadBitstreamOp`\n\nOperator class to read H264 video bit stream.\n\nThis implementation is based on `nvidia::gxf::VideoReadBitStream`.\n\n##### Parameters\n\n- **`output_transmitter`**: Transmitter to send the compressed data\n  - type: `holoscan::IOSpec*`\n- **`input_file_path`**: Path to image file\n  - type: `std::string`\n- **`pool`**: Memory pool for allocating output data\n  - type: `std::shared_ptr<Allocator>`\n- **`outbuf_storage_type`**: Output Buffer storage type, 0:kHost, 1:kDevice\n  - type: `int32_t`\n",
        "application_name": "video_read_bitstream",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "videomaster",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk_version": "0.5",
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Camera",
                "Deltacast"
            ],
            "ranking": 2,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "videomaster",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "# VideoMaster GXF Operator\n\nThis library contains two operators:\n- videomaster_source: get signal from capture card\n- videomaster_transmitter: generate signal\n\nThese operators wrap the GXF extension to provide support for VideoMaster SDK.\n\n## Requirements\n\nThis operator requires the VideoMaster SDK from Deltacast.\n\n## Building the operator\n\nAs part of Holohub, running CMake on Holohub and point to Holoscan SDK install tree.\n\nThe path to the VideoMaster SDK is also mandatory and can be given through the VideoMaster_SDK_DIR parameter.\n",
        "application_name": "videomaster",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "visualizer_icardio",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "video"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### Visualizer iCardio\n\nThe `visualizer_icardio` extension generates the visualization components from the processed results of the plax chamber model.\n\n#### `nvidia::holoscan::multiai::VisualizerICardio`\n\nVisualizer iCardio extension ingests the processed results of the plax chamber model and generates the key points, the key areas and the lines that are transmitted to the HoloViz codelet.\n\n##### Parameters\n\n- **`in_tensor_names_`**: Input tensor names\n  - type: `std::vector<std::string>`\n- **`out_tensor_names_`**: Output tensor names\n  - type: `std::vector<std::string>`\n- **`allocator_`**: Memory allocator\n  - type: `gxf::Handle<gxf::Allocator>`\n- **`receivers_`**: Vector of input receivers. Multiple receivers supported.\n  - type: `HoloInfer::GXFReceivers`\n- **`transmitter_`**: Output transmitter. Single transmitter supported.\n  - type: `HoloInfer::GXFTransmitters`\n",
        "application_name": "visualizer_icardio",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "tensor_to_video_buffer",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Tensor",
                "Video"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### GXF Tensor to VideoBuffer Converter\n\nThe `tensor_to_video_buffer` converts GXF Tensor to VideoBuffer.\n\n#### `holoscan::ops::TensorToVideoBufferOp`\n\nOperator class to convert GXF Tensor to VideoBuffer. This operator is required\nfor data transfer  between Holoscan operators that output GXF Tensor and\nthe other Holoscan Wrapper Operators that understand only VideoBuffer.\nIt receives GXF Tensor as input and outputs GXF VideoBuffer created from it.\n\n##### Parameters\n\n- **`data_in`**: Data in GXF Tensor format\n - type: `holoscan::IOSpec*`\n- **`data_out`**: Data in GXF VideoBuffer format\n - type: `holoscan::IOSpec*`\n- **`in_tensor_name`**: Name of the input tensor\n  - type: `std::string`\n- **`video_format`**: The video format, supported values: \"yuv420\", \"rgb\"\n  - type: `std::string`\n",
        "application_name": "tensor_to_video_buffer",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "v4l2_video_capture",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "video"
            ],
            "ranking": 2,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "v4l2_video_capture",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "### V4L2 Video Capture\n\nThe `v4l2_video_capture` operator provides support for USB and HDMI.\n\n#### `holoscan::ops::V4L2VideoCapture`\n\nThis implementation is based on `nvidia::gxf::V4L2VideoCapture`.\n\n##### Parameters\n\n- **`allocator`**: Output Allocator\n  - type: `holoscan::Allocator*`\n- **`device`**: Path to the V4L2 device\n  - type: `string`\n- **`width`**: Width of the V4L2 image\n  - type: `int32`  \n- **`height`**: Height of the V4L2 image\n  - type: `int32`\n- **`num_buffers_`**: Number of V4L2 buffers to use\n  - type: `int32`    \n- **`pixel_format_`**: Pixel format of capture stream (RGBA32 or YUYV)\n  - type: `string`   ",
        "application_name": "v4l2_video_capture",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "basic_network",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk_version": "0.5",
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Network",
                "Networking",
                "UDP",
                "Socket",
                "IP",
                "TCP"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "basic_network",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "\n### Basic networking operator\n\nThe `basic_network_operator` operator provides a way to send and receive data over Linux sockets. The\ndestination can be on the same machine or over a network. The basic network operator contains separate\noperators for transmit and receive. Users may choose one or the other, or use both in applications \nrequiring bidirectional traffic.\n\nFor TCP sockets the basic network operator only supports a single stream currently. Future versions\nmay expand this to launch multiple threads to listen on different streams.\n\nThe basic networking operators use class names: `BasicNetworkOpTx` and `BasicNetworkOpRx`\n\n#### `nvidia::holoscan::basic_network_operator`\n\nBasic networking operator\n\n##### Receiver Configuration Parameters\n\n- **`batch_size`**: Packets in batch\n  - type: `integer`\n- **`max_payload_size`**: Maximum payload size for a single packet\n  - type: `integer`\n- **`udp_dst_port`**: UDP destination port for packets\n  - type: `integer`\n- **`l4_proto`**: Layer 4 protocol\n  - type: `string` (`udp`/`tcp`)\n- **`ip_addr`**: Destination IP address\n  - type: `string`    \n\n##### Transmitter Configuration Parameters\n\n- **`max_payload_size`**: Maximum payload size for a single packet\n  - type: `integer`\n- **`udp_dst_port`**: UDP destination port for packets\n  - type: `integer`\n- **`l4_proto`**: Layer 4 protocol\n  - type: `string` (`udp`/`tcp`)\n- **`ip_addr`**: Destination IP address\n  - type: `string`    \n- **`min_ipg_ns`**: Minimum inter-packet gap in nanoseconds\n  - type: `integer`  \n\n\n##### Transmitter and Receiver Operator Parameters\n\nThe transmitter and receiver operator both use the `NetworkOpBurstParams` structure as input\nand output to their ports, respectively. `NetworkOpBurstParams` contains the following fields:\n\n- **`data`**: Pointer to batch of packet data\n  - type: `uint8_t *`\n- **`len`**: Length of total buffer in bytes\n  - type: `integer`\n- **`num_pkts`**: Number of packets in batch\n  - type: `integer`\n\nTo receive messages from the Receive operator use the output port `burst_out`.\nTo send messages to the Transmit operator use the input port `burst_in`.",
        "application_name": "basic_network",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "video_write_bitstream",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "video"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### Video Bit Stream Writer\n\nThe `video_write_bitstream` writes bit stream to the disk at specified output\npath.\n\n#### `holoscan::ops::VideoWriteBitstreamOp`\n\nOperator class to write video bit stream to the disk.\n\nThis implementation is based on `nvidia::gxf::VideoWriteBitstream`.\n\n##### Parameters\n\n- **`output_video_path`**: The file path of the output video\n  - type: `std::string`\n- **`frame_width`**: The width of the output video\n  - type: `int`\n- **`frame_height`**: The height of the output video\n  - type: `int`\n- **`inbuf_storage_type`**: Input Buffer storage type, 0:kHost, 1:kDevice\n  - type: `int`\n- **`data_receiver`**: Receiver to get the data\n  - type: `holoscan::IOSpec*`\n",
        "application_name": "video_write_bitstream",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "advanced_network",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Network",
                "Networking",
                "DPDK",
                "UDP",
                "Ethernet",
                "IP",
                "GPUDirect",
                "RDMA"
            ],
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "advanced_network",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "### Advanced Network Operator\n\nThe Advanced Network Operator provides a way for users to achieve the highest throughput and lowest latency \nfor transmitting and receiving Ethernet frames out of and into their operators. Direct access to the NIC hardware\nis available in userspace using this operator, thus bypassing the kernel's networking stack entirely. With a \nproperly tuned system the advanced network operator can achieve hundreds of Gbps with latencies in the low \nmicroseconds. Performance is highly dependent on system tuning, packet sizes, batch sizes, and other factors. \nThe data may optionally be sent to the GPU using GPUDirect to prevent extra copies to and from the CPU.\n\nSince the kernel's networking stack is bypassed, the user is responsible for defining the protocols used\nover the network. In most cases Ethernet, IP, and UDP are ideal for this type of processing because of their \nsimplicity, but any type of protocol can be implemented or used. The advanced network operator\ngives the option to use several primitives to remove the need for filling out these headers for basic packet types, \nbut raw headers can also be constructed.\n\n#### Requirements\n\n- Linux\n- A DPDK-compatible network card. For GPUDirect only NVIDIA NICs are supported\n- System tuning as described below\n- DPDK 22.11 installed with gpudev support compiled in\n- MOFED 5.8-1.0.1.1 or later\n\n#### Features\n\n- **High Throughput**: Hundreds of gigabits per second is possible with the proper hardware\n- **Low Latency**: With direct access to the NIC's ring buffers, most latency incurred is only PCIe latency\n- **GPUDirect**: Optionally send data directly from the NIC to GPU, or directly from the GPU to NIC. GPUDirect has two modes:\n  - Header-data split: Split the header portion of the packet to the CPU and the rest (payload) to the GPU. The split point is\n    configurable by the user. This option should be the preferred method in most cases since it's easy to use and still\n    gives near peak performance.\n  - Batched GPU: Receive batches of whole packets directly into the GPU memory. This option requires the GPU kernel to inspect\n    and determine how to handle packets. While performance may increase slightly over header-data split, this method\n    requires more effort and should only be used for advanced users.\n- **Flow Configuration**: Configure the NIC's hardware flow engine for configurable patterns. Currently only UDP source\n    and destination are supported.\n\n#### Limitations\n\nThe limitations below will be removed in a future release.\n\n- GPUDirect only works in the RX direction. TX will come in a future release\n- GPUDirect only supports header-data split mode\n- Only a single RX and TX DPDK core have been tested\n- Only UDP fill mode is supported\n\n#### Implementation\n\nInternally the advanced network operator is implemented using DPDK. DPDK is an open-source userspace packet processing\nlibrary supported across platforms and vendors. While the DPDK interface is abstracted away from users of the\nadvanced network operator, the method in which DPDK integrates with Holoscan is important for understanding \nhow to achieve the highest performance and for debugging. \n\nWhen the advanced network operator is compiled/linked against a Holoscan application, an instance of the DPDK manager\nis created, waiting to accept configuration. When either an RX or TX advanced network operator is defined in a \nHoloscan application, their configuration is sent to the DPDK manager. Once all advanced network operators have initialized,\nthe DPDK manager is told to initialize DPDK. At this point the NIC is configured using all parameters given by the operators. \nThis step allocates all packet buffers, initializes the queues on the NIC, and starts the appropriate number of internal \nthreads. The job of the internal threads is to take packets off or put packets onto the NIC as fast as possible. They\nact as a proxy between the advanced network operators and DPDK by handling packets faster than the operators may be\nable to.\n\nTo achieve zero copy throughout the whole pipeline only pointers are passed between each entity above. When the user\nreceives the packets from the network operator it's using the same buffers that the NIC wrote to either CPU or GPU \nmemory. This architecture also implies that the user must explicitly decide when to free any buffers it's owning.\nFailure to free buffers will result in errors in the advanced network operators not being able to allocate buffers.\n\n#### System Tuning\n\nFrom a high level, tuning the system for a low latency workload prevents latency spikes large enough to cause anomalies\nin the application. This section details how to perform the basic tuning steps needed on both a Clara AGX and Orin IGX systems.\n\n##### Create Hugepages\n\nHugepages give the kernel access to a larger page size than the default (usually 4K) which reduces the number of memory \ntranslations that have to be actively maintained in MMUs. 1GB hugepages are ideal, but 2MB may be used as well if 1GB is not\navailable. To configure 1GB hugepages:\n\n```\nmkdir /mnt/huge\nmount -t hugetlbfs nodev /mnt/huge\nsudo sh -c \"echo nodev /mnt/huge hugetlbfs pagesize=1GB 0 0 >> /etc/fstab\"\n```\n\n##### Linux Boot Command Line\n\nThe Linux boot command line allows configuration to be injected into Linux before booting. Some configuration options are\nonly available at the boot command since they must be provided before the kernel has started. On the Clara AGX and Orin IGX\nediting the boot command can be done with the following configuration:\n\n```\nvim /boot/extlinux/extlinux.conf\n# Find the line starting with APPEND and add the following\n\n# For Orin IGX:\nisolcpus=6-11 nohz_full=6-11 irqaffinity=0-5 rcu_nocbs=6-11 rcu_nocb_poll tsc=reliable audit=0 nosoftlockup default_hugepagesz=1G hugepagesz=2M hugepages=2\n\n# For Clara AGX:\nisolcpus=4-7 nohz_full=4=7 irqaffinity=0-3 rcu_nocbs=4-7 rcu_nocb_poll tsc=reliable audit=0 nosoftlockup default_hugepagesz=1G hugepagesz=1G hugepages=2\n```\n\nThe settings above isolate CPU cores 6-11 on the Orin and 4-7 on the Clara, and turn 1GB hugepages on.\n\n##### Setting the CPU governor\n\nThe CPU governor reduces power consumption by decreasing the clock frequency of the CPU when cores are idle. While this is useful\nin most environments, increasing the clocks from an idle period can cause long latency stalls. To disable frequency scaling:\n\n```\nsudo apt install cpufrequtils\nsudo sed -i 's/^GOVERNOR=.*/GOVERNOR=\"performance\"/' /etc/init.d/cpufrequtils\n```\n\nReboot the system after these changes.\n\n##### Permissions\n\nDPDK typically requires running as a root user. If you wish to run as a non-root user, you may follow the directions here:\nhttp://doc.dpdk.org/guides/linux_gsg/enable_func.html\n\nIf running in a container, you will need to run in privileged container, and mount your hugepages mount point from above into the container. This\ncan be done as part of the `docker run` command by adding the following flags:\n\n```\n-v /mnt/huge:/mnt/huge \\\n--privileged \\\n```        \n\n#### Configuration Parameters\n\nThe advanced network operator contains a separate operator for both transmit and receive. This allows applications to choose\nwhether they need to handle bidirectional traffic or only unidirectional. Transmit and receive are configured separately in\na YAML file, and a common configuration contains items used by both directions. Each configuration section is described below.\n\n##### Common Configuration\n\nThe common configuration container parameters are used by both TX and RX:\n\n- **`version`**: Version of the config. Only 1 is valid currently.\n  - type: `integer`\n- **`master_core`**: Master core used to fork and join network threads. This core is not used for packet processing and can be\nbound to a non-isolated core\n  - type: `integer`  \n\n##### Receive Configuration\n\n- **`if_name`**: Name of the interface or PCIe BDF to use\n  - type: `string`\n- **`queues`**: Array of queues\n  - type: `array`\n- **`name`**: Name of queue\n  - type: `string`\n- **`gpu_direct`**: GPUDirect is enabled on the queue\n  - type: `boolean`  \n- **`batch_size`**: Number of packets in a batch that is passed between the advanced network operator and the user's operator. A\nlarger number increases throughput and latency by requiring fewer messages between operators, but takes longer to populate a single\nbuffer. A smaller number reduces latency and bandwidth by passing more messages.\n- **`num_concurrent_batches`**: Number of batches that can be outstanding (not freed) at any given time. This value directly affects\nthe amount of memory needed for receiving packets. A value too small and packets will be dropped, while a value too large will\nunnecessarily use excess CPU and/or GPU memory.\n  - type: `integer`\n- **`max_packet_size`**: Largest packet size expected\n  - type: `integer`\n- **`split_boundary`**: Split point in bytes where any byte before this value is sent to CPU, and anything after to GPU\n  - type: `integer` \n- **`gpu_device`**: GPU device number if using GPUDirect\n  - type: `integer` \n- **`cpu_cores`**: List of CPU cores from the isolated set used by the operator for receiving\n  - type: `string`\n- **`flows`**: Array of flows\n  - type: `array`\n- **`name`**: Name of queue\n  - type: `string`\n- **`action`**: Action section of flow\n  - type: `sequence`\n- **`type`**: Type of action. Only \"queue\" is supported currently.\n  - type: `string`\n- **`id`**: ID of queue to steer to\n  - type: `integer`\n- **`match`**: Match section of flow\n  - type: `sequence`\n- **`udp_src`**: UDP source port\n  - type: `integer`\n- **`udp_dst`**: UDP destination port\n  - type: `integer`    \n  \n##### Transmit Configuration\n\n- **`if_name`**: Name of the interface or PCIe BDF to use\n  - type: `string`\n- **`queues`**: Array of queues\n  - type: `array`\n- **`name`**: Name of queue\n  - type: `string`  \n- **`id`**: ID of queue to steer to\n  - type: `integer`  \n- **`gpu_direct`**: GPUDirect is enabled on the queue\n  - type: `boolean`    \n- **`batch_size`**: Number of packets in a batch that is passed between the advanced network operator and the user's operator. A\nlarger number increases throughput and latency by requiring fewer messages between operators, but takes longer to populate a single\nbuffer. A smaller number reduces latency and bandwidth by passing more messages.\n  - type: `integer`\n- **`max_payload_size`**: Largest payload size expected\n  - type: `integer`\n- **`layer_fill`**: Layer(s) that the advanced network operator should populate in the packet. Anything higher than the layer\nspecified must be populated by the user. For example, if `ethernet` is specified, the user is responsible for populating values of\nany item above that layer (IP, UDP, etc...). Valid values are `raw`, `ethernet`, `ip`, and `udp`\n  - type: `string` \n- **`eth_dst_addr`**: Destination ethernet MAC address. Only used for `ethernet` layer_fill mode or above\n  - type: `string` \n- **`ip_src_addr`**: Source IP address to send packets from. Only used for `ip` layer_fill and above\n  - type: `string`\n- **`ip_dst_addr`**: Destination IP address to send packets to. Only used for `ip` layer_fill and above\n  - type: `string`  \n- **`udp_dst_port`**: UDP destination port. Only used for `udp` layer_fill and above\n  - type: `integer`\n- **`udp_src_port`**: UDP source port. Only used for `udp` layer_fill and above\n  - type: `integer`\n- **`cpu_cores`**: List of CPU cores for transmitting\n  - type: `string`\n\n  #### API Structures\n\n  Both the transmit and receive operators use a common structure named `AdvNetBurstParams` to pass data to/from other operators. \n  `AdvNetBurstParams` provides pointers to all packets on the CPU and GPU, and contains metadata needed by the operator to track\n  allocations. Since the advanced network operator utilizes a generic interface that does not expose the underlying low-level network\n  card library, interacting with the `AdvNetBurstParams` is mostly done with the helper functions described below. A user should\n  never modify any members of `AdvNetBurstParams` directly as this may break in future versions. The `AdvNetBurstParams` is described\n  below:\n\n  ```\n  struct AdvNetBurstParams {\n    union {\n        AdvNetBurstParamsHdr hdr;\n        uint8_t buf[HS_NETWORK_HEADER_SIZE_BYTES];\n    };\n\n    void **cpu_pkts;\n    void **gpu_pkts;\n};\n```\n\nStarting from the top, the `hdr` field contains metadata about the batch of packets. `buf` is a placeholder for future expansion\nof fields. `cpu_pkts` contains pointers to CPU packets, while `gpu_pkts` contains pointers to the GPU packets. As mentioned above, \nthe `cpu_pkts` and `gpu_pkts` are opaque pointers and should not be access directly. See the next section for information on interacting\nwith these fields.\n\n#### Example API Usage\n\nFor an entire list of API functions, please see the `adv_network_common.h` header file. \n\n##### Receive\n\nThe section below describes a workflowusing GPUDirect to receive packets using header-data split. The job of the user's operator(s)\nis to process and free the buffers as quickly as possible. This might be copying to interim buffers or freeing before the entire \npipeline is done processing. This allows the networking piece to use relatively few buffers while still achieving very high rates.\n\nThe first step in receiving from the advanced network operator is to tie your operator's input port to the output port of the RX\nnetwork operator's `burst_out` port. \n\n```\nauto adv_net_rx    = make_operator<ops::AdvNetworkOpRx>(\"adv_network_rx\", from_config(\"adv_network_common\"), from_config(\"adv_network_rx\"), make_condition<BooleanCondition>(\"is_alive\", true));\nauto my_receiver   = make_operator<ops::MyReceiver>(\"my_receiver\", from_config(\"my_receiver\"));\nadd_flow(adv_net_rx, my_receiver, {{\"burst_out\", \"burst_in\"}});   \n```\n\nOnce the ports are connected, inside the `compute()` function of your operator you will receive a `AdvNetBurstParams` structure\nwhen a batch is complete:\n\n```\nauto burst = op_input.receive<AdvNetBurstParams>(\"burst_in\");\n```\n\nThe packets arrive in scattered packet buffers. Depending on the application, you may need to iterate through the packets to\naggregate them into a single buffer. Alternatively the operator handling the packet data can operate on a list of packet\npointers rather than a contiguous buffer. Below is an example of aggregating separate GPU packet buffers into a single GPU\nbuffer:\n\n```\n  for (int p = 0; p < adv_net_get_num_pkts(burst); p++) {\n    h_dev_ptrs_[aggr_pkts_recv_ + p]   = adv_net_get_cpu_pkt_ptr(burst, p);\n    ttl_bytes_in_cur_batch_           += adv_net_get_gpu_packet_len(burst, p) + sizeof(UDPPkt);\n  }\n\n  simple_packet_reorder(buffer, h_dev_ptrs, packet_len, burst->hdr.num_pkts); \n```\n\nFor this example we are tossing the header portion (CPU), so we don't need to examine the packets. Since we launched a reorder\nkernel to aggregate the packets in GPU memory, we are also done with the GPU pointers. All buffers may be freed to the\nadvanced network operator at this point:\n\n```\nadv_net_free_all_burst_pkts_and_burst(burst_bufs_[b]);\n```\n\n##### Transmit \n\nTransmitting packets works similar to the receive side, except the user is tasked with filling out the packets as much as it\nneeds to. As mentioned above, helper functions are available to fill in most boilerplate header information if that doesn't \nchange often. \n\nSimilar to the receive, the transmit operator needs to connect to `burst_in` on the advanced network operator transmitter:\n\n```\nauto my_transmitter  = make_operator<ops::MyTransmitter>(\"my_transmitter\", from_config(\"my_transmitter\"), make_condition<BooleanCondition>(\"is_alive\", true));      \nauto adv_net_tx       = make_operator<ops::AdvNetworkOpTx>(\"adv_network_tx\", from_config(\"adv_network_common\"), from_config(\"adv_network_tx\"));\nadd_flow(my_transmitter, adv_net_tx, {{\"burst_out\", \"burst_in\"}});\n```\n\nBefore sending packets, the user's transmit operator must request a buffer from the advanced network operator pool:\n\n```\nauto msg = std::make_shared<AdvNetBurstParams>();\nmsg->hdr.num_pkts = num_pkts;\nif ((ret = adv_net_get_tx_pkt_burst(msg.get())) != AdvNetStatus::SUCCESS) {\n  HOLOSCAN_LOG_ERROR(\"Error returned from adv_net_get_tx_pkt_burst: {}\", static_cast<int>(ret));\n  return;\n}\n```      \n\nThe code above creates a shared `AdvNetBurstParams` that will be passed to the advanced network operator, and uses \n`adv_net_get_tx_pkt_burst` to populate the burst buffers with valid packet buffers. On success, the buffers inside the\nburst structure will be allocate and are ready to be filled in. Each packet must be filled in by the user. In this\nexample we loop through each packet and populate a buffer:\n\n```\nfor (int num_pkt = 0; num_pkt < msg->hdr.num_pkts; num_pkt++) {\n  void *payload_src = data_buf + num_pkt * nom_pkt_size;\n  if (adv_net_set_udp_payload(msg->cpu_pkts[num_pkt], payload_src, nom_pkt_size) != AdvNetStatus::SUCCESS) {\n    HOLOSCAN_LOG_ERROR(\"Failed to create packet {}\", num_pkt);\n  }\n}\n```\n\nThe code iterates over `msg->hdr.num_pkts` (defined by the user) and passes a pointer to the payload and the packet\nsize to `adv_net_set_udp_payload`. In this example our configuration is using `fill_mode` \"udp\" on the transmitter, so\n`adv_net_set_udp_payload` will populate the Ethernet, IP, and UDP headers. The payload pointer passed by the user\nis also copied into the buffer. Alternatively a user could use the packet buffers directly as output from a previous stage\nto avoid this extra copy.\n\nWith the `AdvNetBurstParams` populated, the burst can be sent off to the advanced network operator for transmission:\n\n```\nop_output.emit(msg, \"burst_out\");\n```\n",
        "application_name": "advanced_network",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "emergent_source",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Camera",
                "Emergent"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "emergent_source",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "# HoloHub Operators\n\nThis directory contains operators for the Holoscan Platform.\n\n# Contributing to HoloHub Operators\n\nPlease review the [CONTRIBUTING.md file](https://github.com/nvidia-holoscan/holohub/blob/main/CONTRIBUTING.md) guidelines to contribute operators.\n",
        "application_name": "emergent_source",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "video_encoder",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "video",
                "encoder"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "videoencoder",
                        "version": "1.0"
                    },
                    {
                        "name": "videoencoderio",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "### Video Encoder\n\nThe `video_encoder` encodes YUV video frames to h264 bit stream.\n\n#### `holoscan::ops::VideoEncoderOp`\n\nOperator class to encode YUV video frames to h264 bit stream.\n\nThis implementation is based on `nvidia::gxf::VideoEncoder`.\n\n##### Parameters\n\n- **`input_frame`**: Receiver to get the input frame\n  - type: `holoscan::IOSpec*`\n- **`output_transmitter`**: Transmitter to send the compressed data\n  - type: `holoscan::IOSpec*`\n- **`pool`**: Memory pool for allocating output data\n  - type: `std::shared_ptr<Allocator>`\n- **`inbuf_storage_type`**: Input Buffer storage type, 0:kHost, 1:kDevice\n  - type: `int32_t`\n- **`outbuf_storage_type`**: Output Buffer storage type, 0:kHost, 1:kDevice\n  - type: `int32_t`\n- **`device`**: Path to the V4L2 device. Default:\"/dev/nvidia0\"\n  - type: `std::string`\n- **`codec`**: Video Codec to use,  0:H264, only H264 supported. Default:0\n  - type: `int32_t`\n- **`input_height`**: Input frame height\n  - type: `uint32_t`\n- **`input_width`**: Input image width\n  - type: `uint32_t`\n- **`input_format`**: Input frame color format, nv12 PL is supported. Default:\"nv12pl\"\n  - type: `std::string`\n- **`profile`**: Encode profile, 0:Baseline Profile, 1: Main , 2: High\n  - type: `int32_t`\n- **`bitrate`**: Encoder bitrate, Bitrate of the encoded stream, in bits per second. Default:20000000\n  - type: `int32_t`\n- **`framerate`**: Frame Rate, FPS. Default:30\n  - type: `int32_t`\n",
        "application_name": "video_encoder",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "video_decoder",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "video",
                "decoder"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "videodecoder",
                        "version": "1.0"
                    },
                    {
                        "name": "videodecoderio",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "### Video Decoder\n\nThe `video_decoder` decodes h264 bit stream to YUV.\n\n#### `holoscan::ops::VideoDecoderOp`\n\nOperator class to decode h264 bit stream to YUV.\n\nThis implementation is based on `nvidia::gxf::VideoDecoder`.\n\n##### Parameters\n\n- **`image_receiver`**: Receiver to get the input image\n  - type: `holoscan::IOSpec*`\n- **`output_transmitter`**: Transmitter to send the decoded data\n  - type: `holoscan::IOSpec*`\n- **`pool`**: Memory pool for allocating output data\n  - type: `std::shared_ptr<Allocator>`\n- **`inbuf_storage_type`**: Input Buffer storage type, 0:kHost, 1:kDevice\n  - type: `int32_t`\n- **`outbuf_storage_type`**: Output Buffer storage type, 0:kHost, 1:kDevice\n  - type: `int32_t`\n- **`device`**: Path to the V4L2 device. Default:\"/dev/nvidia0\"\n  - type: `std::string`\n- **`codec`**: Video codec,  0:H264, only H264 supported. Default:0\n  - type: `int32_t`\n",
        "application_name": "video_decoder",
        "source_folder": "operators"
    }
]