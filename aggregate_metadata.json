[{"application": {"name": "High Speed Endoscopy", "authors": [{"name": "Holoscan Team", "affiliation": "NVIDIA"}], "language": "Python", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk": {"minimum_required_version": "0.5.0", "tested_versions": ["0.5.0"]}, "platforms": ["amd64", "arm64"], "tags": ["Camera", "Emergent"], "ranking": 1, "dependencies": {"gxf_extensions": [{"name": "emergent_source", "version": "1.0"}]}, "run": {"command": "python3 <holohub_app_source>/high_speed_endoscopy.py", "workdir": "holohub_bin"}}, "readme": "# High-Speed Endoscopy\n\nThe application showcases how high resolution cameras can be used to capture the scene, post-processed on GPU and displayed at high frame rate.\n\n### Requirements\n\nThis application requires:\n1. an Emergent Vision Technologies camera (see [setup instructions]((https://docs.nvidia.com/clara-holoscan/sdk-user-guide/emergent_setup.html)\n2. a NVIDIA ConnectX SmartNIC with Rivermax SDK and drivers installed (see [prerequisites](../../README.md#prerequisites))\n3. a display with high refresh rate to keep up with the camera's framerate\n4. [additional setups](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/additional_setup.html) to reduce latency\n\n### Run Instructions\n\nTODO\n", "application_name": "high_speed_endoscopy"}, {"application": {"name": "High Speed Endoscopy", "authors": [{"name": "Holoscan Team", "affiliation": "NVIDIA"}], "language": "CPP", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk": {"minimum_required_version": "0.5.0", "tested_versions": ["0.5.0"]}, "platforms": ["amd64", "arm64"], "tags": ["Camera", "Emergent"], "ranking": 1, "dependencies": {"gxf_extensions": [{"name": "emergent_source", "version": "1.0"}]}, "run": {"command": "<holohub_app_bin>/high_speed_endoscopy", "workdir": "holohub_bin"}}, "readme": "# High-Speed Endoscopy\n\nThe application showcases how high resolution cameras can be used to capture the scene, post-processed on GPU and displayed at high frame rate.\n\n### Requirements\n\nThis application requires:\n1. an Emergent Vision Technologies camera (see [setup instructions]((https://docs.nvidia.com/clara-holoscan/sdk-user-guide/emergent_setup.html)\n2. a NVIDIA ConnectX SmartNIC with Rivermax SDK and drivers installed (see [prerequisites](../../README.md#prerequisites))\n3. a display with high refresh rate to keep up with the camera's framerate\n4. [additional setups](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/additional_setup.html) to reduce latency\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n> \u26a0\ufe0f At this time, camera controls are hardcoded within the `gxf_emergent_source` extension. To update them at the application level, the GXF extension, and the application need to be rebuilt.\nFor more information on the controls, refer to the [EVT Camera Attributes Manual](https://emergentvisiontec.com/resources/?tab=umg)\n\n### Run Instructions\n\nFirst, go in your `build` or `install` directory. Then, run the commands of your choice:\n\n* RDMA disabled\n    ```bash\n    # C++\n    sed -i -e 's#rdma:.*#rdma: false#' ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy.yaml \\\n        && sudo ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy\n    ```\n\n* RDMA enabled\n    ```bash\n    # C++\n    sed -i -e 's#rdma:.*#rdma: true#' ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy.yaml \\\n        && sudo MELLANOX_RINGBUFF_FACTOR=14 ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy\n    ```\n\n> \u2139\ufe0f The `MELLANOX_RINGBUFF_FACTOR` is used by the EVT driver to decide how much BAR1 size memory would be used on the dGPU. It can be changed to different number based on different use cases.\n", "application_name": "high_speed_endoscopy"}, {"application": {"name": "Videomaster transmitter example", "authors": [{"name": "Laurent Radoux", "affiliation": "DELTACAST"}], "language": "C++", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk_version": "0.5.0", "platforms": ["amd64", "arm64"], "tags": ["Deltacast", "VideoMaster"], "ranking": 2, "dependencies": {}, "run": {"command": "<holohub_app_bin>/deltacast_transmitter --data <holohub_data_dir>/endoscopy", "workdir": "holohub_bin"}}, "readme": "# Deltacast transmitter\n\nThis application demonstrates the use of videomaster_transmitter to transmit a video stream through a dedicated IO device.\n\n### Requirements\n\nThis application uses the DELTACAST.TV capture card for input stream. Contact [DELTACAST.TV](https://www.deltacast.tv/) for more details on how access the SDK and to setup your environment.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nSee instructions from the top level README on how to build this application.\nNote that this application requires to provide the VideoMaster_SDK_DIR if it is not located in a default location on the sytem.\nThis can be done with the following command, from the top level Holohub source directory:\n\n```bash\n./run build deltacast_transmitter --configure-args -DVideoMaster_SDK_DIR=<Path to VideoMasterSDK>\n```\n\n### Run Instructions\n\nFrom the build directory, run the command:\n\n```bash\n./applications/deltacast_transmitter/deltacast_transmitter --data <holohub_data_dir>/endoscopy\n```\n", "application_name": "deltacast_transmitter"}, {"application": {"name": "Software Defined Radio FM Demodulation", "authors": [{"name": "Adam Thompson", "affiliation": "NVIDIA"}], "language": "Python", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk_version": "0.4.0", "platforms": ["amd64"], "tags": ["Communications, Aerospace, Defence, Lifesciences"], "ranking": 2, "dependencies": {"libraries": [{"name": "numpy", "version": "1.23.2"}, {"name": "cupy", "version": "11.4"}, {"name": "cusignal", "version": "22.12"}, {"name": "SoapySDR", "version": "0.8.1"}, {"name": "soapysdr-module-rtlsdr", "version": "0.3"}, {"name": "pyaudio", "version": "0.2.13"}]}, "run": {"command": "python3 sdr_fm_demodulation.py", "workdir": "holohub_app_source"}}, "readme": "# SDR FM Demodulation Application\n\nAs the \"Hello World\" application of software defined radio developers, this demonstration highlights real-time FM demodulation, resampling, and playback on GPU with NVIDIA's Holoscan SDK. In this example, we are using an inexpensive USB-based [RTL-SDR](https://www.rtl-sdr.com/) dongle to feed complex valued Radio Frequency (RF) samples into GPU memory and use [cuSignal](https://github.com/rapidsai/cusignal) functions to perform the relevant signal processing. The main objectives of this demonstration are to:\n- Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries\n- Demonstrate how to construct and connect isolated units of work via Holoscan operators\n- Emphasize that operators created for this application can be re-used in other ones doing similar tasks\n\n# Running the Application\n\nPrior to running the application, the user needs to install the necessary dependencies (and, of course, plug in a SDR into your computer). This is most easily done in an Anaconda environment.\n\n```\nconda create --name holoscan-sdr-demo python=3.8\nconda activate holoscan-sdr-demo\nconda install -c conda-forge -c rapidsai -c nvidia cusignal soapysdr soapysdr-module-rtlsdr pyaudio\npip install holoscan\n```\n\nThe FM demodulation example can then be run via\n```\npython applications/sdr_fm_demodulation/sdr_fm_demodulation.py\n```\n", "application_name": "sdr_fm_demodulation"}, {"application": {"name": "H264 Endoscopy Tool Tracking", "authors": [{"name": "Holoscan Team", "affiliation": "NVIDIA"}], "language": "CPP", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk": {"minimum_required_version": "0.5.0", "tested_versions": ["0.5.0"]}, "platforms": ["amd64", "arm64"], "tags": ["Endoscopy", "Video Encoding"], "ranking": 1, "dependencies": {"operators": [{"name": "videodecoder", "version": "1.0"}, {"name": "videoencoder", "version": "1.0"}]}, "run": {"command": "./h264_endoscopy_tool_tracking h264_endoscopy_tool_tracking.yaml --data <holohub_data_dir>/endoscopy", "workdir": "holohub_app_bin"}}, "readme": "# H264 Endoscopy Tool Tracking Application\n\nThe application showcases how to use H.264 video source as input to and output\nfrom the Holoscan pipeline. This application is a modified version of Endoscopy\nTool Tracking reference application in Holoscan SDK that supports H.264\nelementary streams as the input and output.\n\n_The H.264 video decode operator does not adjust framerate as it reads the elementary\nstream input. As a result the video stream will be displayed as quickly as the decoding can be\nperformed. This feature will be coming soon to a new version of the operator._\n\n## Requirements\n\nThis application is configured to use H.264 elementary stream from endoscopy\nsample data as input. The output of the pipeline is again recorded to a H.264\nelementary stream on the disk, file name / path for this can be specified in\nthe 'h264_endoscopy_tool_tracking.yaml' file.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nUnzip the sample data:\n\n```\nunzip holoscan_endoscopy_sample_data_20230128.zip -d <data_dir>\n```\n\n## Building the application\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n## Running the application\n\nRun the application `h264_endoscopy_tool_tracking` in the binary directory.\n\n```bash\ncd <build_dir>/applications/h264_endoscopy_tool_tracking/ \\\n  && ./h264_endoscopy_tool_tracking --data <HOLOHUB_DATA_DIR>/endoscopy\n```\n", "application_name": "h264_endoscopy_tool_tracking"}, {"application": {"name": "H.264 Video Decode Reference Application", "authors": [{"name": "Holoscan Team", "affiliation": "NVIDIA"}], "language": "CPP", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk": {"minimum_required_version": "0.5.0", "tested_versions": ["0.5.0"]}, "platforms": ["amd64", "arm64"], "tags": ["H.264", "Video Decoding"], "ranking": 1, "dependencies": {"operators": [{"name": "videodecoder", "version": "1.0"}, {"name": "videodecoderio", "version": "1.0"}]}, "run": {"command": "./h264_video_decode h264_video_decode.yaml --data <holohub_data_dir>/endoscopy", "workdir": "holohub_app_bin"}}, "readme": "# H.264 Video Decode Reference Application\n\nThis is a minimal reference application demonstrating usage of H.264 video\ndecode operator. This application makes use of H.264 elementary stream reader\noperator for reading H.264 elementary stream input and uses Holoviz operator\nfor rendering decoded data to the native window.\n\n_The H.264 video decode operator does not adjust framerate as it reads the elementary\nstream input. As a result the video stream will be displayed as quickly as the decoding can be\nperformed. This feature will be coming soon to a new version of the operator._\n\n## Requirements\n\nThis application is configured to use H.264 elementary stream from endoscopy\nsample data as input. To use any other stream, the filename / path for the\ninput file can be specified in the 'h264_video_decode.yaml' file.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nUnzip the sample data:\n\n```\nunzip holoscan_endoscopy_sample_data_20230128.zip -d <data_dir>\n```\n\n## Building the application\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n## Running the application\n\nRun the application `h264_video_decode` in the binary directory.\n\n```bash\ncd <build_dir>/applications/h264_video_decode && ./h264_video_decode --data <HOLOHUB_DATA_DIR>/endoscopy\n```\n", "application_name": "h264_video_decode"}, {"application": {"name": "Endoscopy Tool Segmentation from MONAI Model Zoo Application", "authors": [{"name": "Jin Lin", "affiliation": "NVIDIA"}], "language": "Python", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk_version": "0.5.0", "platforms": ["amd64, arm64"], "tags": ["MONAI", "Endoscopy", "Segmentation"], "ranking": 2, "dependencies": {"libraries": []}}, "readme": "# Endoscopic Tool Segmentation from MONAI Model Zoo\nThis endoscopy tool segmentation application runs the MONAI Endoscopic Tool Segmentation from [MONAI Model Zoo](https://github.com/Project-MONAI/model-zoo/tree/dev/models/endoscopic_tool_segmentation).\n\n\nThis HoloHub application has been verified on the GI Genius sandbox and is currently deployable to GI Genius Intelligent Endoscopy Modules. [GI Genius](https://www.cosmoimd.com/gi-genius/) is Cosmo Intelligent Medical Devices\u2019 AI-powered endoscopy system. This implementation by Cosmo Intelligent Medical Devices showcases the fast and seamless deployment of HoloHub applications on products/platforms running on NVIDIA Holoscan.\n\n## Model\nWe will be deploying the endoscopic tool segmentation model from [MONAI Model Zoo](https://github.com/Project-MONAI/model-zoo/tree/dev/models/endoscopic_tool_segmentation). <br>\nNote that you could also use the MONAI model zoo repo for training your own semantic segmentation model with your own data, but here we are directly deploying the downloaded MONAI model checkpoint into Holoscan. \n\n\n### Model conversion to ONNX\nBefore deploying the MONAI Model Zoo's trained model checkpoint in Holoscan SDK, we convert the model checkpoint into ONNX. <br>\nYou can choose to \n- download the [already converted ONNX model here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/monai_endoscopic_tool_segmentation_model) directly and skip the rest of this Model section, or \n- go through the following conversion steps yourself. \n\n 1. Download the PyTorch model checkpoint linked in the README of [endoscopic tool segmentation](https://github.com/Project-MONAI/model-zoo/tree/dev/models/endoscopic_tool_segmentation#model-overview). We will assume its name to be `model.pt`.\n 2. Clone the MONAI Model Zoo repo. \n```\ncd [your-workspace]\ngit clone https://github.com/Project-MONAI/model-zoo.git\n```\nand place the downloaded PyTorch model into `model-zoo/models/endoscopic_tool_segmentation/`.\n\n 3. Pull and run the docker image for [MONAI](https://hub.docker.com/r/projectmonai/monai). We will use this docker image for converting the PyTorch model to ONNX. \n```\ndocker pull projectmonai/monai\ndocker run -it --rm --gpus all -v [your-workspace]/model-zoo:/workspace/model-zoo -w /workspace/model-zoo/models/endoscopic_tool_segmentation/ projectmonai/monai\n```\n 4. Install onnxruntime within the container\n ```\npip install onnxruntime onnx-graphsurgeon\n ```\n 5. Convert model\n \nWe will first export the model.pt file to ONNX by using the [export_to_onnx.py](https://github.com/Project-MONAI/model-zoo/blob/dev/models/endoscopic_tool_segmentation/scripts/export_to_onnx.py) file. Modify the backbone in [line 122](https://github.com/Project-MONAI/model-zoo/blob/dev/models/endoscopic_tool_segmentation/scripts/export_to_onnx.py#L122) to be efficientnet-b2:\n```\nmodel = load_model_and_export(modelname, outname, out_channels, height, width, multigpu, backbone=\"efficientnet-b2\")\n```\nNote that the model in the Model Zoo here was trained to have only two output channels: label 1 = tools, label 0 = everything else, but the same Model Zoo repo can be repurposed to train a model with a different dataset that has more than two classes.\n```\npython scripts/export_to_onnx.py --model model.pt --outpath model_endoscopic_tool_seg.onnx --width 736 --height 480 --out_channels 2\n```\nFold constants in the ONNX model.\n```\npolygraphy surgeon sanitize --fold-constants model_endoscopic_tool_seg.onnx -o model_endoscopic_tool_seg_sanitized.onnx\n```\nFinally, modify the input and output channels to have shape [n, height, width, channels], [n, channels, height, width]. \n```\npython scripts/graph_surgeon_tool_seg.py --orig_model model_endoscopic_tool_seg_sanitized.onnx --new_model model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx\n```\n\n## Data\nFor this application we will use the same [Endoscopy Sample Data](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data) as the Holoscan SDK reference applications.\n\n## Requirements\nThe only requirement is to make sure the model and data are accessible by the application. If running inside a Docker container, please mount the path(s) where the model and data are.\n## Running the application\nIf running within a Docker container, while launching the container, make sure this directory containing applications and the directory containing models are mounted in the runtime container. Add the `-v` mount options to the `docker run` command.\n\nMake sure the [yaml file](./tool_segmentation.yaml) is pointing to the right locations for the ONNX model and data. The assumption in the yaml file is that the converted ONNX model is located at:\n```\nmodel_file_path: /byom/models/tool_seg/model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx\nengine_cache_dir: /byom/models/tool_seg/model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out_engines\n```\nand the [Endoscopy Sample Data](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data) is assumed to be at \n```\n/workspace/holoscan-sdk/data/endoscopy\n```\n\nPlease check and modify the paths to model and data in the yaml file if needed.\n\nRun the Python application simply by:\n```\npython tool_segmentation.py\n```\n", "application_name": "monai_endoscopic_tool_seg"}, {"application": {"name": "MultiAI Ultrasound", "authors": [{"name": "Holoscan Team", "affiliation": "NVIDIA"}], "language": "Python", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk": {"minimum_required_version": "0.5.0", "tested_versions": ["0.5.0"]}, "platforms": ["amd64", "arm64"], "tags": ["Ultrasound", "MultiAI"], "ranking": 1, "dependencies": {}, "run": {"command": "python3 <holohub_app_source>/multiai_ultrasound.py  --data <holohub_data_dir>/multiai_ultrasound", "workdir": "holohub_app_bin"}}, "readme": "# Multi-AI Ultrasound\n\nThis application demonstrates how to run multiple inference pipelines in a single application by leveraging the Holoscan Inference module, a framework that facilitates designing and executing inference applications in the Holoscan SDK.\n\nThe Multi AI operators (inference and postprocessor) use APIs from the Holoscan Inference module to extract data, initialize and execute the inference workflow, process, and transmit data for visualization.\n\nThe applications uses models and echocardiogram data from iCardio.ai. The models include:\n- a Plax chamber model, that identifies four critical linear measurements of the heart\n- a Viewpoint Classifier model, that determines confidence of each frame to known 28 cardiac anatomical view as defined by the guidelines of the American Society of Echocardiography\n- an Aortic Stenosis Classification model, that provides a score which determines likeability for the presence of aortic stenosis\n\nThe default configuration (`multiai_ultrasound.yaml`) runs on default GPU (GPU-0). Multi-AI Ultrasound application can be executed on multiple GPUs with the Holoscan SDK version 0.6 onwards. A sample configuration file for multi GPU configuration for multi-AI ultrasound application (`mgpu_multiai_ultrasound.yaml`) is present in both `cpp` and `python` applications. The multi-GPU configuration file is designed for a system with at least 2 GPUs connected to the same PCIE network.\n\n### Requirements\n\n- Python 3.8+\n- The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the echocardiogram (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for Multi-AI Ultrasound Pipeline](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_multi_ai_ultrasound_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\nNext, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/multiai_ultrasound/python\n    python3 multiai_ultrasound.py --source=replayer --data <DATA_DIR>/multiai_ultrasound\n    ```\n\n* Using a pre-recorded video on multi-GPU system\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/multiai_ultrasound/python\n    python3 multiai_ultrasound.py --config mgpu_multiai_ultrasound.yaml --source=replayer --data <DATA_DIR>/multiai_ultrasound\n    ```\n\n* Using an AJA card\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/multiai_ultrasound/python\n    python3 multiai_ultrasound.py --source=aja\n    ```\n", "application_name": "multiai_ultrasound"}, {"application": {"name": "MultiAI Ultrasound", "authors": [{"name": "Holoscan Team", "affiliation": "NVIDIA"}], "language": "CPP", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk": {"minimum_required_version": "0.5.0", "tested_versions": ["0.5.0"]}, "platforms": ["amd64", "arm64"], "tags": ["Ultrasound", "MultiAI"], "ranking": 1, "dependencies": {}, "run": {"command": "<holohub_app_bin>/multiai_ultrasound --data <holohub_data_dir>/multiai_ultrasound", "workdir": "holohub_app_bin"}}, "readme": "# Multi-AI Ultrasound\n\nThis application demonstrates how to run multiple inference pipelines in a single application by leveraging the Holoscan Inference module, a framework that facilitates designing and executing inference applications in the Holoscan SDK.\n\nThe Inference and the Processing operators use APIs from the Holoscan Inference module to extract data, initialize and execute the inference workflow, process, and transmit data for visualization.\n\nThe applications uses models and echocardiogram data from iCardio.ai. The models include:\n- a Plax chamber model, that identifies four critical linear measurements of the heart\n- a Viewpoint Classifier model, that determines confidence of each frame to known 28 cardiac anatomical view as defined by the guidelines of the American Society of Echocardiography\n- an Aortic Stenosis Classification model, that provides a score which determines likeability for the presence of aortic stenosis\n\nThe default configuration (`multiai_ultrasound.yaml`) runs on default GPU (GPU-0). Multi-AI Ultrasound application can be executed on multiple GPUs with the Holoscan SDK version 0.6 onwards. A sample configuration file for multi GPU configuration for multi-AI ultrasound application (`mgpu_multiai_ultrasound.yaml`) is present in both `cpp` and `python` applications. The multi-GPU configuration file is designed for a system with at least 2 GPUs connected to the same PCIE network.\n\n### Requirements\n\nThe provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the echocardiogram (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for Multi-AI Ultrasound Pipeline](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_multi_ai_ultrasound_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n### Run Instructions\n\nIn your `build` directory, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    sed -i -e 's#^source:.*#source: replayer#' applications/multiai_ultrasound/cpp/multiai_ultrasound.yaml\n    applications/multiai_ultrasound/cpp/multiai_ultrasound --data <DATA_DIR>/multiai_ultrasound\n    ```\n\n* Using a pre-recorded video on multi-GPU system\n    ```bash\n    sed -i -e 's#^source:.*#source: replayer#' applications/multiai_ultrasound/cpp/mgpu_multiai_ultrasound.yaml\n    applications/multiai_ultrasound/cpp/multiai_ultrasound applications/multiai_ultrasound/cpp/mgpu_multiai_ultrasound.yaml --data <DATA_DIR>/multiai_ultrasound\n    ```\n\n* Using an AJA card\n    ```bash\n    sed -i -e 's#^source:.*#source: aja#' applications/multiai_ultrasound/cpp/multiai_ultrasound.yaml\n    applications/multiai_ultrasound/cpp/multiai_ultrasound\n    ```\n", "application_name": "multiai_ultrasound"}, {"application": {"name": "Power Spectral Denisty with cuNumeric", "authors": [{"name": "Adam Thompson", "affiliation": "NVIDIA"}], "language": "Python", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk": {"minimum_required_version": "0.5.0", "tested_versions": ["0.5.0"]}, "platforms": ["amd64"], "tags": ["Life Sciences, Aerospace, Defense, Communications"], "ranking": 2, "dependencies": {"libraries": [{"name": "numpy", "version": "1.24.2"}, {"name": "cupy", "version": "12.0"}, {"name": "cunumeric", "version": "23.03"}]}}, "readme": "# Calculate Power Spectral Density with Holoscan and cuNumeric\n\n[cuNumeric](https://github.com/nv-legate/cunumeric) is an drop-in replacement for NumPy that aims to provide a distributed and accelerated drop-in replacement for the NumPy API on top of the [Legion](https://legion.stanford.edu/) runtime. It works best for programs that have very large arrays of data that can't fit in the the memory of a single GPU or node.\n\nIn this example application, we are using the cuNumeric library within a Holoscan application graph to determine the Power Spectral Density (PSD) of an incoming signal waveform. Notably, this is simply achived by taking the absolute value of the FFT of a data array.\n\n The main objectives of this demonstration are to:\n- Highlight developer productivity in building an end-to-end streaming application with Holoscan and cuNumeric\n- Demonstrate how to scale a given workload to multiple GPUs using cuNumeric\n\n# Running the Application\n\nPrior to running the application, the user needs to install the necessary dependencies. This is most easily done in an Anaconda environment.\n\n```\nconda create --name holoscan-cunumeric-demo python=3.9\nconda activate holoscan-cunumeric-demo\nconda install -c nvidia -c conda-forge -c legate cunumeric cupy\npip install holoscan\n```\n\nThe cuNumeric PSD processing pipeline example can then be run via\n```\nlegate --gpus 2 applications/cunumeric_integration/cunumeric_psd.py\n```\n\nWhile running the application, you can confirm multi GPU utilization via watching `nvidia-smi` or using another GPU utilization tool\n\nTo run the same application without cuNumeric, simply change `import cunumeric as np` to `import cupy as np` in the code and run\n```\npython applications/cunumeric_integration/cunumeric_psd.py\n```\n", "application_name": "cunumeric_integration"}, {"application": {"name": "Advanced Networking Benchmark", "authors": [{"name": "Cliff Burdick", "affiliation": "NVIDIA"}], "language": "CPP", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "platforms": ["amd64", "arm64"], "tags": ["Network", "Networking", "DPDK", "UDP", "Ethernet", "IP", "GPUDirect", "RDMA"], "holoscan_sdk": {"minimum_required_version": "0.5.0", "tested_versions": ["0.5.0"]}, "ranking": 1, "dependencies": {"gxf_extensions": [{"name": "advanced_networking_benchmark", "version": "1.0"}]}, "run": {"command": "<holohub_app_bin>/adv_networking_bench", "workdir": "holohub_bin"}}, "readme": "# Advanced Networking Benchmark\n\nThis is a simple application to measure a lower bound on performance for the advanced networking operator\nby receiving packets, optionally doing work on them, and freeing the buffers. While only freeing the packets is\nan unrealistic workload, it's useful to see at a high level whether the application is able to keep up with\nthe bare minimum amount of work to do. The application contains both a transmitter and receiver that are\ndesigned to run on different systems, and may be configured independently.\n\nThe performance of this application depends heavily on a properly-configured system and choosing the best\ntuning parameters that are acceptable for the workload. To configure the system please see the documentation\nfor the advanced network operator. With the system tuned, the application performance will be dictated\nby batching size and whether GPUDirect is enabled. \n\nAt this time both the transmitter and receiver are written to handle an Ethernet+IP+UDP packet with a\nconfigurable payload. Other modes may be added in the future. Also, for simplicity, the transmitter and\nreceiver must be configured to a single packet size.\n\n## Transmit\n\nThe transmitter sends a UDP packet with an incrementing sequence of bytes after the UDP header. The batch\nsize configured dictates how many packets the benchmark operator sends to the advanced network operator\nin each tick. Typically with the same number of CPU cores the transmitter will run faster than the receiver, \nso this parameter may be used to throttle the sender somewhat by making the batches very small.\n\n## Receiver\n\nThe receiver receives the UDP packets in either CPU-only mode or header-data split mode. CPU-only mode\nwill receive the packets in CPU memory, copy the payload contents to a host-pinned staging buffer, and\nfreed. In header-data split mode the user may configure a split point where the bytes before that point\nare sent to the CPU, and all bytes afterwards are sent to the GPU. Header-data split should achieve higher\nrates than CPU mode since the amount of data to the CPU can be orders of magnitude lower compared to running\nin CPU-only mode. \n\n### Configuration\n\nThe application is configured using a separate transmit and receive file. The transmit file is called\n`adv_networking_bench_tx.yaml` while the receive is named `adv_networking_bench_rx.yaml`. Configure the\nadvanced networking operator on both transmit and receive per the instructions for that operator.\n\n#### Receive Configuration\n\n- `header_data_split`: bool\n  Turn on GPUDirect header-data split mode\n- `batch_size`: integer\n  Size in packets for a single batch. This should be a multiple of the advanced network RX operator batch size.\n  A larger batch size consumes more memory since any work will not start unless this batch size is filled. Consider\n  reducing this value if errors are occurring.\n- `max_packet_size`: integer\n  Maximum packet size expected. This value includes all headers up to and including UDP.\n\n#### Transmit Configuration\n\n- `batch_size`: integer\n  Size in packets for a single batch. This batch size is used to send to the advanced network TX operator, and \n  will loop sending that many packets for each burst.\n- `payload_size`: integer\n  Size of the payload to send after all L2-L4 headers \n\n### Requirements\n\nThis application requires all configuration and requirements from the advanced network operator.\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n### Run Instructions\n\nFirst, go in your `build` or `install` directory, then for the transmitter run:\n\n\n```bash\n./build/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_tx.yaml\n```\n\nOr for the receiver:\n\n```bash\n./build/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_tx.yaml\n```\n", "application_name": "adv_networking_bench"}, {"application": {"name": "Colonoscopy segmentation", "authors": [{"name": "Holoscan Team", "affiliation": "NVIDIA"}], "language": "Python", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk": {"minimum_required_version": "0.5.0", "tested_versions": ["0.5.0"]}, "platforms": ["amd64", "arm64"], "tags": ["Colonoscopy", "Classification"], "dependencies": {"data": "https://ngc.nvidia.com/resources/colonoscopy_sample_app_data"}, "run": {"command": "python3 colonoscopy_segmentation.py --data <holohub_data_dir>/colonoscopy_segmentation", "workdir": "holohub_app_source"}}, "readme": "# Colonoscopy Polyp Segmentation\n\nFull workflow including a generic visualization of segmentation results from a polyp segmentation models.\n\n### Requirements\n\n- Python 3.8+\n- The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the ultrasound data (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI Colonoscopy Segmentation of Polyps](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_colonoscopy_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\nNext, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/colonoscopy_segmentation\n    python3 colonoscopy_segmentation.py --source=replayer --data=<DATA_DIR>/colonoscopy_segmentation\n    ```\n\n* Using an AJA card\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/colonoscopy_segmentation\n    python3 colonoscopy_segmentation.py --source=aja\n    ```\n\n### Holoscan SDK version\n\nColonoscopy segmentation application in HoloHub requires version 0.6+ of the Holoscan SDK.\nIf the Holoscan SDK version is 0.5 or lower, following code changes must be made in the application:\n\n* In python/CMakeLists.txt: update the holoscan SDK version from `0.6` to `0.5`\n* In python/multiai_ultrasound.py: `InferenceOp` is replaced with `MultiAIInferenceOp`\n", "application_name": "colonoscopy_segmentation"}, {"application": {"name": "Speech-to-text + Large Language Model", "authors": [{"name": "Sean Huver", "affiliation": "NVIDIA"}], "language": "Python", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk": {"minimum_required_version": "0.5.0", "tested_versions": ["0.5.0"]}, "platforms": ["amd64"], "tags": ["Speech-to-text", "Large Language Model"], "ranking": 2, "dependencies": {"openai-whisper": "^20230314", "openai": "^0.27.2"}, "run": {"command": "python3 stt_to_nlp.py", "workdir": "holohub_app_source"}}, "readme": "# HoloHub Applications\n\nThis directory contains applications based on the Holoscan Platform.\nSome applications might require specific hardware and software packages which are described in the \nmetadata.json and/or README.md for each application.\n\n# Contributing to HoloHub Applications\n\nPlease review the [CONTRIBUTING.md file](https://github.com/nvidia-holoscan/holohub/blob/main/CONTRIBUTING.md) guidelines to contribute applications.\n", "application_name": "speech_to_text_llm"}, {"application": {"name": "Endoscopy Out of Body Detection Pipeline in C++", "authors": [{"name": "Holoscan Team", "affiliation": "NVIDIA"}], "language": "C++", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk": {"minimum_required_version": "0.5.0", "tested_versions": ["0.5.0"]}, "platforms": ["amd64", "arm64"], "tags": ["Endoscopy", "Classification"], "dependencies": {"model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/endoscopy_out_of_body_detection", "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/endoscopy_out_of_body_detection"}, "run": {"command": "./endoscopy_out_of_body_detection endoscopy_out_of_body_detection.yaml --data <holohub_data_dir>/endoscopy_out_of_body_detection", "workdir": "holohub_app_bin"}}, "readme": "# Endoscopy Out of Body Detection Application\n\nThis application performs endoscopy out of body detection. The application classifies if the input frame is inside the body or out of the body. If the input frame is inside the body, application prints `Likely in-body`, otherwise `Likely out-of-body`. Each likelihood is accompanied with a confidence score. \n\n__Note: there is no visualization component in the application.__\n\n`endoscopy_out_of_body_detection.yaml` is the configuration file. Input video file is converted into GXF tensors and the name and location of the GXF tensors are updated in the `basename` and the `directory` field in `replayer`.\n\n## Data\n\n__Note: the data is automatically downloaded and converted when building. If you need to manually convert the data follow the following steps.__\n\n\n* Endoscopy out of body detection model and the sample dataset is available on [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/endoscopy_out_of_body_detection)\n  After downloading the data in mp4 format, it must be converted into GXF tensors.\n* Script for GXF tensor conversion (`convert_video_to_gxf_entities.py`) is available with the Holoscan SDK, and can be accessed [here](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts)\n\n### Unzip and convert the sample data:\n\n```\n# unzip the downloaded sample data\nunzip [FILE].zip -d <data_dir>\n\n# convert video file into tensor\nffmpeg -i <INPUT_VIDEO_FILE> -fs 900M -pix_fmt rgb24 -f rawvideo pipe:1 | python convert_video_to_gxf_entities.py --width 256 --height 256 --channels 3 --framerate 30\n\n# where <INPUT_VIDEO_FILE> is one of the downloaded MP4 files: OP1-out-2.mp4, OP4-out-8.mp4 or OP8-out-4.mp4.\n```\n\nMove the model file and converted video tensor into a directory structure similar to the following:\n\n```bash\ndata\n\u2514\u2500\u2500 endoscopy_out_of_body_detection\n    \u251c\u2500\u2500 LICENSE.md\n    \u251c\u2500\u2500 out_of_body_detection.onnx\n    \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_entities\n    \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_index\n    \u2514\u2500\u2500 sample_clip_out_of_body_detection.mp4\n```\n\n## Building the application\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n## Running the application\n\nIn your `build` directory, run\n\n```bash\napplications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection --data ../data/endoscopy_out_of_body_detection\n```\n", "application_name": "endoscopy_out_of_body_detection"}, {"application": {"name": "SSD Detection Application", "authors": [{"name": "Jin Lin", "affiliation": "NVIDIA"}], "language": "Python", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk_version": "0.5.0", "platforms": ["amd64, arm64"], "tags": ["SSD", "bounding box", "Detection"], "ranking": 2, "dependencies": {"libraries": [{"name": "numpy", "version": "1.22.3"}, {"name": "cupy", "version": "11.3.0"}]}}, "readme": "# SSD Detection Application\n## Model\nWe can train the [SSD model from NVIDIA DeepLearningExamples repo]((https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD)) with any data of our choosing. Here for the purpose of demonstrating the deployment process, we will use a SSD model checkpoint that is only trained for the [demo video clip](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data). \n\nPlease download the models [at this NGC Resource](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/ssd_surgical_tool_detection_model) for `epoch_24.pt`, `epoch24_nms.onnx` and `epoch24.onnx`. You can go through the next steps of Model Conversion to ONNX to convert `epoch_24.pt` into `epoch24_nms.onnx` and `epoch24.onnx`, or use the downloaded ONNX models directly.\n\n\n### Model Conversion to ONNX\nThe scripts we need to export the model from .pt checkpoint to the ONNX format are all within this dir `./scripts`. It is a two step process.\n\n\n Step 1: Export the trained checkpoint to ONNX. <br> We use [`export_to_onnx_ssd.py`](./scripts/export_to_onnx_ssd.py) if we want to use the model as is without NMS, or [`export_to_onnx_ssd_nms.py`](./scripts/export_to_onnx_ssd_nms.py) to prepare the model with NMS. \n Let's assume the re-trained SSD model checkpoint from the [repo](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD) is saved as `epoch_24.pt`.\n The export process is \n```\n# For exporting the original ONNX model\n python export_to_onnx_ssd.py --model epoch_24.pt  --outpath epoch24_temp.onnx\n```\n```\n# For preparing to add the NMS step to ONNX model\npython export_to_onnx_ssd_nms.py --model epoch_24.pt  --outpath epoch24_nms_temp.onnx\n```\nStep 2: modify input shape. <br> Step 1 produces a onnx model with input shape `[1, 3, 300, 300]`, but we will want to modify the input node to have shape `[1, 300, 300, 3]` or in general `[batch_size, height, width, channels]` for compatibility and easy of deployment in the Holoscan SDK. If we want to incorporate the NMS operation in the the ONNX model, we could add a `EfficientNMS_TRT` op, which is documented in [`graph_surgeon_ssd.py`](./scripts/graph_surgeon_ssd.py)'s nms related block.\n```\n# For exporting the original ONNX model\npython graph_surgeon_ssd.py --orig_model epoch24_temp.onnx --new_model epoch24.onnx\n```\n```\n# For adding the NMS step to ONNX model, use --nms\npython graph_surgeon_ssd.py --orig_model epoch24_nms_temp.onnx --new_model epoch24_nms.onnx --nms\n```\n\nNote that\n - `epoch24.onnx` is used in `ssd_step1.py` and `ssd_step2_route1.py`\n - `epoch24_nms.onnx` is used in `ssd_step2_route2.py` and `ssd_step2_route2_render_labels.py` \n\n## Data\nFor this application we will use the same [Endoscopy Sample Data](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data) as the Holoscan SDK reference applications.\n\n## Requirements\nThere are two requirements \n1. To run `ssd_step1.py` and `ssd_step2_route1.py` with the original exported model, we need the installation of PyTorch and CuPy. <br> To run `ssd_step2_route2.py` and `ssd_step2_route2_render_labels.py` with the exported model with additional NMS layer in ONNX, we need the installation of CuPy. <br>If you choose to build the SDK from source, you can find the [modified Dockerfile here](./docker/Dockerfile) to replace the SDK repo [Dockerfile](https://github.com/nvidia-holoscan/holoscan-sdk/blob/main/Dockerfile) to satisfy the installation requirements. \n<br>\nThe main changes in Dockerfile: \n<br>(1) the base image changed to `nvcr.io/nvidia/pytorch:22.03-py3` instead of the `nvcr.io/nvidia/tensorrt:22.03-py3`. This is mainly for the PyTorch dependency in `ssd_step1.py` and `ssd_step2_route1.py` while running on aarch64 developer kits, as there are no compatible PyTorch pip wheels available for aarch64. <br>(2) added the installation of NVTX for profiling.\n<br>Then, build the dev container following the [README instructions](https://github.com/nvidia-holoscan/holoscan-sdk#recommended-using-the-run-script).<br>\nWhen launching the container, make sure this directory containing applications and the directory containing models are mounted in the runtime container. Add the `-v` mount options to the `docker run` command.\n\n2. Make sure the model and data are accessible by the application. \n<br> Make sure the yaml file ssd_endo_model.yaml and ssd_endo_model_with_NMS.yaml are pointing to the right locations for the ONNX model and data. The assumption in the yaml file is that the `epoch24_nms.onnx` and `epoch24.onnx` are located at:\n```\nmodel_file_path: /byom/models/endo_ssd/epoch24_nms.onnx \nengine_cache_dir: /byom/models/endo_ssd/epoch24_nms_engines\n```\nand / or\n```\nmodel_file_path: /byom/models/endo_ssd/epoch24.onnx\nengine_cache_dir: /byom/models/endo_ssd/epoch24_engines\n```\nThe [Endoscopy Sample Data](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data) is assumed to be at \n```\n/workspace/holoscan-sdk/data/endoscopy\n```\nPlease check and modify the paths to model and data in the yaml file if needed.\n\n## Building the application\nPlease refer to the README under [./app_dev_process](./app_dev_process/README.md) to see the process of building the applications.\n\n## Running the application\nRun the incrementally improved Python applications by:\n```\npython ssd_step1.py\n\npython ssd_step2_route1.py\n\npython ssd_step2_route2.py\n\npython ssd_step2_route2_render_labels.py --labelfile endo_ref_data_labels.csv\n```", "application_name": "ssd_detection_endoscopy_tools"}, {"application": {"name": "Videomaster endoscopy example application", "authors": [{"name": "Laurent Radoux", "affiliation": "DELTACAST"}], "language": "C++", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk_version": "0.5.0", "platforms": ["amd64", "arm64"], "tags": ["DeltaCast", "VideoMaster", "Endoscopy", "Classification"], "ranking": 3, "dependencies": {"model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data", "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data"}, "run": {"command": "<holohub_app_bin>/deltacast_endoscopy_tool_tracking --data <holohub_data_dir>/endoscopy", "workdir": "holohub_bin"}}, "readme": "# Deltacast Endoscopy Tool Tracking\n\nThis application application is based on the endoscopy_tool_tracking application and demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream using Deltacast's videomaster SDK.\n\n### Requirements\n\nThis application uses the DELTACAST.TV capture card for input stream. Contact [DELTACAST.TV](https://www.deltacast.tv/) for more details on how access the SDK and to setup your environment.\n\n### Data\n\nThis applications uses the dataset from the endoscopy tool tracking application:\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\n### Build Instructions\n\nSee instructions from the top level README on how to build this application.\nNote that this application requires to provide the VideoMaster_SDK_DIR if it is not located in a default location on the sytem.\nThis can be done with the following command, from the top level Holohub source directory:\n\n```bash\n./run build deltacast_endoscopy_tool_tracking --configure-args -DVideoMaster_SDK_DIR=<Path to VideoMasterSDK>\n```\n\n### Run Instructions\n\nFrom the build directory, run the command:\n\n```bash\n./applications/deltacast_endoscopy_tool_tracking/deltacast_endoscopy_tool_tracking --data <holohub_data_dir>/endoscopy\n```\n", "application_name": "deltacast_endoscopy_tool_tracking"}, {"application": {"name": "USB/HDMI Video Capture", "authors": [{"name": "Mikael Brudfors", "affiliation": "NVIDIA"}], "language": "Python", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk": {"minimum_required_version": "0.5.0", "tested_versions": ["0.5.0"]}, "platforms": ["amd64", "arm64"], "tags": ["Camera"], "ranking": 2, "dependencies": {"gxf_extensions": [{"name": "v4l2_video_capture", "version": "1.0"}]}, "run": {"command": "python3 <holohub_app_source>/usb_hdmi_video_capture.py", "workdir": "holohub_bin"}}, "readme": "# Capture USB and HDMI Video Stream\n\nThis app captures either a USB or an HDMI video stream and visualizes it using Holoviz.\n\n## Requirements\n\nInstall the following dependency:\n```sh\nsudo apt-get install libv4l-dev=1.18.0-2build1\n```\n\nNote that you might not have permissions to open the video device(s), run:\n```sh\n sudo chmod 666 /dev/video?\n ```\n to make them available.\n\n## Parameters\n\nMake sure that the `pixel_format` parameter in the YAML config file is set correctly, options are `RGBA32` and `YUYV`. Also make sure that the `device` parameter is set to the mount point of the device you want to stream from. \n\nThese parameters can be found with:\n```sh\nv4l2-ctl --list-devices\n```\nfollowed by:\n```sh\nv4l2-ctl -d /dev/video0 --list-formats-ext\n```\nIf you do not have the `v4l2-ctl` app, it can be installed with:\n```sh\nsudo apt-get install v4l-utils\n```\n\n## HDMI IN\n\nThe HDMI IN on the dev kit will have to be activated in order for capturing to work. Please look at the relevant dev kit user guide for instructions. Additionally, the parameter `pixel_format` needs to be set to `RGBA32` and  the `device` parameter set to the mount point of the HDMI IN device.\n\n## Run Instructions\n\nFirst, build the app with the root folder `run.sh` script:\n```sh\n./run build usb_hdmi_video_capture\n```\n\n### C++\n\n```sh\ncd build/applications/usb_hdmi_video_capture/cpp\n./usb_hdmi_video_capture\n```\n\n### Python\n\n```sh\ncd applications/usb_hdmi_video_capture/python/\npython3 usb_hdmi_video_capture.py\n```", "application_name": "usb_hdmi_video_capture"}, {"application": {"name": "USB/HDMI Video Capture", "authors": [{"name": "Mikael Brudfors", "affiliation": "NVIDIA"}], "language": "CPP", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk": {"minimum_required_version": "0.5.0", "tested_versions": ["0.5.0"]}, "platforms": ["amd64", "arm64"], "tags": ["Camera"], "ranking": 2, "dependencies": {"gxf_extensions": [{"name": "v4l2_video_capture", "version": "1.0"}]}, "run": {"command": "<holohub_app_bin>/usb_hdmi_video_capture", "workdir": "holohub_bin"}}, "readme": "# Capture USB and HDMI Video Stream\n\nThis app captures either a USB or an HDMI video stream and visualizes it using Holoviz.\n\n## Requirements\n\nInstall the following dependency:\n```sh\nsudo apt-get install libv4l-dev=1.18.0-2build1\n```\n\nNote that you might not have permissions to open the video device(s), run:\n```sh\n sudo chmod 666 /dev/video?\n ```\n to make them available.\n\n## Parameters\n\nMake sure that the `pixel_format` parameter in the YAML config file is set correctly, options are `RGBA32` and `YUYV`. Also make sure that the `device` parameter is set to the mount point of the device you want to stream from. \n\nThese parameters can be found with:\n```sh\nv4l2-ctl --list-devices\n```\nfollowed by:\n```sh\nv4l2-ctl -d /dev/video0 --list-formats-ext\n```\nIf you do not have the `v4l2-ctl` app, it can be installed with:\n```sh\nsudo apt-get install v4l-utils\n```\n\n## HDMI IN\n\nThe HDMI IN on the dev kit will have to be activated in order for capturing to work. Please look at the relevant dev kit user guide for instructions. Additionally, the parameter `pixel_format` needs to be set to `RGBA32` and  the `device` parameter set to the mount point of the HDMI IN device.\n\n## Run Instructions\n\nFirst, build the app with the root folder `run.sh` script:\n```sh\n./run build usb_hdmi_video_capture\n```\n\n### C++\n\n```sh\ncd build/applications/usb_hdmi_video_capture/cpp\n./usb_hdmi_video_capture\n```\n\n### Python\n\n```sh\ncd applications/usb_hdmi_video_capture/python/\npython3 usb_hdmi_video_capture.py\n```", "application_name": "usb_hdmi_video_capture"}, {"application": {"name": "Basic Networking Ping", "authors": [{"name": "Cliff Burdick", "affiliation": "NVIDIA"}], "language": "CPP", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk_version": "0.5.0", "platforms": ["amd64", "arm64"], "tags": ["Networking", "Network", "UDP", "IP"], "ranking": 1, "dependencies": {"gxf_extensions": [{"name": "basic_network", "version": "1.0"}]}, "run": {"command": "<holohub_app_bin>/basic_networking_ping", "workdir": "holohub_bin"}}, "readme": "# Basic Networking Ping\n\nThis application takes the existing ping example that runs over Holoscan ports and instead uses the basic\nnetwork operator to run over a UDP socket.\n\nThe basic network operator allows users to send and receive UDP messages over a standard Linux socket.\nSeparate transmit and receive operators are provided so they can run independently and better suit\nthe needs of the application.\n\n### Configuration\n\nThe application is configured using the file basic_networking_ping.yaml. Depending on how the machine\nis configured, the IP and UDP port likely need to be configured. All other settings do not need to be\nchanged.\n\nPlease refer to the basic network operator documentation for more configuration information\n\n### Requirements\n\nThis application requires:\n1. Linux\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n### Run Instructions\n\nFirst, go in your `build` or `install` directory. Then, run the commands of your choice:\n\n\n```bash\n./build/applications/basic_networking_ping/cpp/basic_networking_ping\n```\n", "application_name": "basic_networking_ping"}, {"application": {"name": "Simple Classical Radar Pipeline", "authors": [{"name": "Cliff Burdick", "affiliation": "NVIDIA"}], "language": "Python", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk_version": "0.4.0", "platforms": ["amd64"], "tags": ["Aerospace, Defense, Communications"], "ranking": 2, "dependencies": {"libraries": [{"name": "numpy", "version": "1.23.2"}, {"name": "cupy", "version": "11.4"}, {"name": "cusignal", "version": "22.12"}]}, "run": {"command": "python3 simple_radar_pipeline.py", "workdir": "holohub_app_source"}}, "readme": "# Simple Radar Pipeline Application\n\nThis demonstration walks the developer through building a simple radar signal processing pipeline, targeted towards detecting objects, with Holoscan. In this example, we generate random radar and waveform data, passing both through:\n1. Pulse Compression\n2. Moving Target Indication (MTI) Filtering\n3. Range-Doppler Map\n4. Constant False Alarm Rate (CFAR) Analysis\n\nWhile this example generates 'offline' complex-valued data, it could be extended to accept streaming data from a phased array system or simulation via modification of the `SignalGeneratorOperator`.\n\nThe output of this demonstration is a measure of the number of pulses per second processed on GPU.\n\n The main objectives of this demonstration are to:\n- Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries\n- Demonstrate how to construct and connect isolated units of work via Holoscan operators, particularly with handling multiple inputs and outputs into an Operator\n- Emphasize that operators created for this application can be re-used in other ones doing similar tasks\n\n# Running the Application\n\nPrior to running the application, the user needs to install the necessary dependencies. This is most easily done in an Anaconda environment.\n\n```\nconda create --name holoscan-sdr-demo python=3.8\nconda activate holoscan-sdr-demo\nconda install -c conda-forge -c rapidsai -c nvidia cusignal\npip install holoscan\n```\n\nThe simple radar signal processing pipeline example can then be run via\n```\npython applications/simple_radar_pipeline/simple_radar_pipeline.py\n```\n", "application_name": "simple_radar_pipeline"}, {"application": {"name": "Simple Radar Pipeline in C++", "authors": [{"name": "Cliff Burdick", "affiliation": "NVIDIA"}], "language": "C++", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk_version": "0.4.0", "platforms": ["amd64", "arm64"], "tags": ["Signal Processing", "RADAR"], "ranking": 2, "dependencies": {"libraries": [{"name": "MatX", "version": "0.2.5", "url": "https://github.com/NVIDIA/MatX.git"}]}, "run": {"command": "<holohub_app_bin>/simple_radar_pipeline", "workdir": "holohub_app_bin"}}, "readme": "# Simple Radar Pipeline Application\n\nThis demonstration walks the developer through building a simple radar signal processing pipeline, targeted towards detecting objects, with Holoscan. In this example, we generate random radar and waveform data, passing both through:\n1. Pulse Compression\n2. Moving Target Indication (MTI) Filtering\n3. Range-Doppler Map\n4. Constant False Alarm Rate (CFAR) Analysis\n\nWhile this example generates 'offline' complex-valued data, it could be extended to accept streaming data from a phased array system or simulation via modification of the `SignalGeneratorOperator`.\n\nThe output of this demonstration is a measure of the number of pulses per second processed on GPU.\n\n The main objectives of this demonstration are to:\n- Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries\n- Demonstrate how to construct and connect isolated units of work via Holoscan operators, particularly with handling multiple inputs and outputs into an Operator\n- Emphasize that operators created for this application can be re-used in other ones doing similar tasks\n\n## Building the application\nMake sure CMake (https://www.cmake.org) is installed on your system (minimum version 3.20)\n\n- [Holoscan Debian Package](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_dev_deb) - Follow the instructions in the link to install the latest version of Holoscan Debian package from NGC.\n\n- Create a build directory:\n  ```bash\n  mkdir -p <build_dir> && cd <build_dir>\n  ```\n- Configure with CMake:\n\n  Make sure CMake can find your installation of the Holoscan SDK. For example, setting `holoscan_ROOT` to its install directory during configuration:\n\n  ```bash\n  cmake -S <source_dir> -B <build_dir> -DAPP_simple_radar_pipeline=1 \n  ```\n\n  _Notes:_\n  _If the error `No CMAKE_CUDA_COMPILER could be found` is encountered, make sure that the :code:`nvcc` executable can be found by adding the CUDA runtime location to your `PATH` variable:_\n\n  ```\n  export PATH=$PATH:/usr/local/cuda/bin\n  ```\n\n- Build:\n\n  ```bash\n  cmake --build <build_dir>\n  ```\n\n## Running the application\n```bash\n<build_dir>/simple_radar_pipeline\n```\n\n", "application_name": "simple_radar_pipeline"}, {"application": {"name": "Endoscopy Tool Tracking", "authors": [{"name": "Holoscan Team", "affiliation": "NVIDIA"}], "language": "Python", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk": {"minimum_required_version": "0.5.0", "tested_versions": ["0.5.0"]}, "platforms": ["amd64", "arm64"], "tags": ["Endoscopy", "Tracking", "AJA"], "ranking": 1, "dependencies": {"model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data", "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data"}, "run": {"command": "python3 <holohub_app_source>/endoscopy_tool_tracking.py --data <holohub_data_dir>/endoscopy", "workdir": "holohub_bin"}}, "readme": "# Endoscopy Tool Tracking\n\nBased on a LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.\n\n### Requirements\n\n- Python 3.8+\n- The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded endoscopy video (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n \n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n \nNext, run the commands of your choice:\n\nThis application should **be run in the build directory of Holohub** in order to load the GXF extensions.\nAlternatively, the relative path of the extensions in the corresponding yaml file can be modified to match path of\nthe working directory.\n\n* Using a pre-recorded video\n    ```bash\n    cd <HOLOHUB_BUILD_DIR>\n    python3 <HOLOHUB_SOURCE_DIR>/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=replayer --data=<DATA_DIR>/endoscopy\n    ```\n\n* Using an AJA card\n    ```bash\n    cd <HOLOHUB_BUILD_DIR>\n    python3  <HOLOHUB_SOURCE_DIR>/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=aja\n    ```\n", "application_name": "endoscopy_tool_tracking"}, {"application": {"name": "Endoscopy Tool Tracking", "authors": [{"name": "Holoscan Team", "affiliation": "NVIDIA"}], "language": "CPP", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk": {"minimum_required_version": "0.5.0", "tested_versions": ["0.5.0"]}, "platforms": ["amd64", "arm64"], "tags": ["Endoscopy", "Tracking", "AJA"], "ranking": 1, "dependencies": {"model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data", "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data"}, "run": {"command": "<holohub_app_bin>/endoscopy_tool_tracking --data <holohub_data_dir>/endoscopy", "workdir": "holohub_bin"}}, "readme": "# Endoscopy Tool Tracking\n\nBased on a LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.\n\n### Requirements\n\nThe provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded endoscopy video (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n### Run Instructions\n\nIn your `build` directory, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    sed -i -e 's#^source:.*#source: replayer#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data <data_dir>/endoscopy\n    ```\n\n* Using an AJA card\n    ```bash\n    sed -i -e 's#^source:.*#source: aja#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n    ```\n", "application_name": "endoscopy_tool_tracking"}, {"application": {"name": "Ultrasound Segmentation", "authors": [{"name": "Holoscan Team", "affiliation": "NVIDIA"}], "language": "Python", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk": {"minimum_required_version": "0.5.0", "tested_versions": ["0.5.0"]}, "platforms": ["amd64", "arm64"], "tags": ["Ultrasound", "Segmentation"], "ranking": 1, "dependencies": {}, "run": {"command": "python3 <holohub_app_source>/ultrasound_segmentation.py  --data <holohub_data_dir>/ultrasound_segmentation", "workdir": "holohub_bin"}}, "readme": "# Ultrasound Bone Scoliosis Segmentation\n\nFull workflow including a generic visualization of segmentation results from a spinal scoliosis segmentation model of ultrasound videos. The model used is stateless, so this workflow could be configured to adapt to any vanilla DNN model. \n\n### Requirements\n\n- Python 3.8+\n- The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the ultrasound data (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Bone Scoliosis Segmentation](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_ultrasound_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\nNext, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/ultrasound_segmentation/python\n    python3 ultrasound_segmentation.py --source=replayer --data <DATA_DIR>/ultrasound_segmentation\n    ```\n\n* Using an AJA card\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/ultrasound_segmentation/python\n    python3 ultrasound_segmentation.py --source=aja\n    ```\n", "application_name": "ultrasound_segmentation"}, {"application": {"name": "Ultrasound Segmentation", "authors": [{"name": "Holoscan Team", "affiliation": "NVIDIA"}], "language": "CPP", "version": "1.0", "changelog": {"1.0": "Initial Release"}, "holoscan_sdk": {"minimum_required_version": "0.5.0", "tested_versions": ["0.5.0"]}, "platforms": ["amd64", "arm64"], "tags": ["Ultrasound", "Segmentation"], "ranking": 1, "dependencies": {}, "run": {"command": "<holohub_app_bin>/ultrasound_segmentation  --data <holohub_data_dir>/ultrasound_segmentation", "workdir": "holohub_bin"}}, "readme": "# Ultrasound Bone Scoliosis Segmentation\n\nFull workflow including a generic visualization of segmentation results from a spinal scoliosis segmentation model of ultrasound videos. The model used is stateless, so this workflow could be configured to adapt to any vanilla DNN model. \n\n### Requirements\n\nThe provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the ultrasound data (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Bone Scoliosis Segmentation](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_ultrasound_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n\n### Run Instructions\n\nIn your `build` directory, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    sed -i -e 's#^source:.*#source: replayer#' applications/ultrasound_segmentation/cpp/ultrasound_segmentation.yaml\n    applications/ultrasound_segmentation/cpp/ultrasound_segmentation --data <data_dir>/ultrasound_segmentation\n    ```\n\n* Using an AJA card\n    ```bash\n    sed -i -e 's#^source:.*#source: aja#' applications/ultrasound_segmentation/cpp/ultrasound_segmentation.yaml\n    applications/ultrasound_segmentation/cpp/ultrasound_segmentation\n    ```\n", "application_name": "ultrasound_segmentation"}]